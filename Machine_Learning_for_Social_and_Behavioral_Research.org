#+TITLE: Machine Learning for Social and Behavioral Research
#+AUTHOR: Christopher Carbonaro
#+EMAIL: ccarbona@nd.edu
#+TODO: TODO IN_PROGRESS | WAITING DONE
#+CATEGORY: ML
#+Professor: Dr. Ross Jacobucci
#+CourseListing: PSY-40122; PSY-60122; MDSC-40122
#+OfficeRmNm: Corbett E440
#+OfficeHrs: Wed. 10:30-11:30
* Introduction and Housekeeping
  <2019-08-27 Tue 09:20-10:45>

** TODO Reading and reflections                                    :Readings:
   DEADLINE: <2019-10-24 Thu +1w>
   :PROPERTIES:
   :LAST_REPEAT: [2019-10-10 Thu 10:29]
   :END:

   - State "DONE"       from "TODO"       [2019-10-10 Thu 10:29]
   - State "DONE"       from "TODO"       [2019-09-12 Thu 10:29]
   - State "DONE"       from "TODO"       [2019-09-04 Wed 23:44]
** Machine Learning Terminology
   Note that we could have used any of the following terms:
   - Statistical Learning
   - Data Mining
   - Artificial Intelligence
   - Exploratory Data Mining
   - Big Data
   - Data Science
     
     Machine learning is not an algorithm (you already know this); it's a larger concept which can be applied in multiple different ways.
   
*** Algorithm vs Model
    Model in statistics typically refers to a "general model" which conveys how the data were generated. Think of it as:
    - Algorithm = Method used: Linear regression, random forests, decision trees: $Y = f(X) + e$
    - Model = Resulting relationship between X and Y after fitting (algorithmic model): $Y = \hat{Y} + e$

*** Exploratory Data Analysis
    ^ A term attributed to John Tukey's book. What if you don't begin your analysis with a hypothesis? This is in contrast to how research was conducted earlier i.e. as "Confirmatory Research." ML is not explanatory. It tries to get at /exactly/ what the causal factors are, but this leads to very complicated explanations.

*** Regression vs Classification
    - Regression: The outcome of interest is continuous. Either integer or interval (metric)
    - Classification: The outcome of interest is categorical. Either binary or multicategorical.

      Complications:
      Likert type variables - Just refer to as ordinal, but more classification. Can produce predicted probabilities (logistic regression) or produce class predictions (clustering).

      N.B. The model can't think. If your measurments are poor measurments, you will get poorly modeled results.

*** X's and Y's
    Most models will have a form of:
    $$Y = f(X) + e$$
    
    Right hand side:
    - X's
    - Predictors
    - Covariates
    - Inputs

    Left hand side:
    - Y
    - Outcome
    - Response

*** Do we have a Y?
    One major distinction in ML is between supervised and unsupervised learning.
    
    - Supervised: These is an outcome
    - Unsupervised: There isn't an outcome of interest
    - Semi-Supervised: Neural Network type learning

    Generally, unsupervised means we are doing data reduction on the X's.

*** Univariate vs Multivariate
    - Univariate: a single outcome
    - Multivariate: multiple outcomes
    - Multivariable: multiple predictors

    Most things in this course are univariate and multivariable, e.g. 
    $$Y = b_0 + b_1 X_1 + b_2 X_2 + e$$

*** Inference
    Inference is typically reserved for models that make assumptions about a population (asymptotics). Mostly this occurs in the context of linear models.

    An analogous term is /explanation/: We want to derive a model to explain the relaitonship between X's and Y.


    This is in contrast to purely predictive modeling (forecasting), often denoted by the use of "black box" algorithms.

*** Parametric vs Non-Parametric
    - Parametric: Making distributional assumptions and using a finitenumber of parameters.
    - Non-Parametric: Everything else, typically encapsulating most ofmachine learning

    Why make assumptions?
    - Requires a smaller sample size
    - Often faster
    - More interpretable
*** Bayes vs Frequentist
    The distinction of Bayes vs Frequentist when it comes to p-values, regression, and such is very different than in ML.

    In ML, there is less of a debate. Bayesian is mostly used to estimate the coefficients of the model, often times faster than traditional (frequentist) approaches. In ML, priors are typically less important and have less of an impact on the results (posterior).
** What is Big Data?
   <2019-08-29 Thu 09:30-10:45>
*** "Is my data big enough?"
    99% of the time: Yes and No. It will be large enough to use the methods but small enough to incur many difficulties.
   
    Beware overfitting (where you get answers which are not generalizing to other samples).
*** Cattell's Data Box
    Time is becoming increasingly important as a variable.

    We used to have problems with keeping study participants over long spans of time. This is now easier with ease of communication between the conductor of the study and the recipient. But time can even be near-continuous as a variable now, with constant updates for variables like location.
*** Big N, Small P
    This is traditional social and behavioral research: N > P.

    Let T represent time.
    If T = 1, this is cross-sectional.
    If T > 1, the study is longitudinal.
*** Small N, Big P
    This falls in line with genetics and neuro research.
    - Lare numbers of SNP's
    - Large numbers of voekls from MRI.

    This work presents a number of problems for traditional statistical algorithms. You /cannot/ run a linear regression on a sample where N < P.
*** Small N, Small P
    Both N and P don't provide enough information. So we needd large T (time).
  
    N = 1, idiographic approach.
    N > 1, ecological momentary assessment or other.
   
    Traditional statistical models need N = big (longitudinal), but recently it has been possible to do studies on an individual level (idiographic) by using large T. This gives the researcher enough observations to begin drawing conclusions.
*** Sources of Data
    ML algorithms allow us to analyze data from new sources:
    - genetics
    - neuro
    - text
    - social media

    One perspective: These new forms of assessment with replace traditional forms.
    Better prespective: These new forms of assessment will /supplement/ traditional forms.

    The same is true for ML vs Statistics -- ML can supplement our traditional approaches.
** TODO Final Project Proposal
   DEADLINE: <2019-11-05 Tue 23:59>
* Week 2: Theory and ML
  <2019-09-03 Tue 09:30-10:45> 
** Exploratory and Confirmatory Research
*** Confirmatory Research
    Definition: A common view of what constitutes confirmatory research is a seriesof a priori hypotheses followed by research design (experimental inmost cases) to test the hypotheses, gathering data, analysis,concluded with inductive inference (Jaeger & Halliday, 1998)

    - Confirmatory – Minimize Type I errors.
      - Goal is to find causal mechanisms. Stringent alpha values.
    - Exploratory – Minimize Type II errors. 
      
    Confirmatory research is a hallmark of science. Stating hypotheses,running a study or experiment to test these hypotheses, and theneither finding enough evidence to support or fail to support thehypothesis, is an efficient and important cornerstone to this practice.

    Think of confirmatory as working from theory to data, whereas exploratory can be thought of as working from data towards theory.

*** Confirmatory Applied to Statistical Methods
    In defining confirmatory research, it is common to apply this to specific algorithms/models:
    - ANOVA
    - Mediation Models
    - Structural Equation Models
    - Propensity Score Matching
    The common theme is that these are often defined in terms of /causal/ modeling.
*** Modeling vs Practices
    Confirmatory models can be used in exploratory ways.

    Theoretical modeling - how much theory is incorporated into the model being tested?
    Theoretical practices - how much does theory inform every other part? e.g. data cleaning, whether hypotheses come before analysis, etc...
*** Confirmation and the Replication Crisis
    Replication crisis -> New emphasis on confirmatory research practices (e.g., Wagenmakers et al., 2012).

    Biggest Takeaways:
    - Pre-registration
    - Reporting all analyses
    - Not generating hypotheses from the data analysis
    - Posting data and analysis scripts
    - Pre-stating what confounders (or control variables) are used

    Lieberman & Cunningham (2009) found the average issue of JPSP had an average of 93 statistical tests per paper.

*** Exploratory Research
    Definition: One consistent distinction is in the types of data aligned with each type of modeling, with confirmatory data analysis (CDA) using mostly experimental data, while exploratory data analysis (EDA) typically uses observational data (Good, 1983).

    EDA is mostly attributed to Tukey - non-hypothesis informed graphical exploration of data to tell a story.

*** Generating Hypotheses
    EDA is more concerend with observable data, as opposed to those from experiements (Good, 1983) - obtained informally, thus the methods are often more informal.

    EDA is mostly concerned with the /encouragement/ of hypothesis formulation (Good, 1983).

*** EDA Can Improve Hypotheses
    Even if a hypothesis is formed, tested, and not rejected, there is almost always room for improvement.

    Examples:
    - Model the residuals
    - Follow up traditional statistics with ML

    EDA is sometimes likened to detective work (per Tukey) as a mechanism to uncover those things missed through more restrictive methods.

*** Motivation for Exploratory Research
    Jaeger & Halliday (1998) - "Explicit hypotheses tested with confirmatory research usually do not spring from an intellectual void but instead are gained through exploratory research."

    Some sects of Philosophy of science hold this form of notion (hypothetico deductivists) -> "Thinking really really hard"

*** Using ML to Inform Confirmatory Research
    Using twitter to assess moral content on twitter, followed by an experiment that manipulated perceived moral differences (Dehghani et al., 2016).

    - First step: Machine learning identifies the most important aspects - Theory generation.
    - Second step: Follow up with theory-testing in the form of an experiment or observational study.

*** Splitting the Data
    One way to use the same data for EDA and CDA is to split it prior to analysis
    - EDA on first half of sample - can torture as much as you want.
    - CDA on second half using hypotheses generated on first
      
    *N.B.* This cuts your data set in half, reducing your statistical power.

    According to Prof. Jacobuccui, this is a bad idea. You are less likely to find something in the first place.

*** Data Reduction
    Descriptive statistics is one form of data reduction: "We must suppress some of the truth to communicate the truth" (p 286; Good, 1983).

    Using of plotting techniques such as stem-and-leaf or histogram, or something like PCA:

*** Wonky Statistics
    There is nothing wrong with EDA - Problems occur when EDA is passed off as CDA.

    Example: A p-value works under the assumption that this is the single test being used.

    E.g., Torturing the data until it confesses - and it will confess.

** Culture Shift
   <2019-09-05 Thu 09:30-10:45>
** DONE Homework Assignment 1
   DEADLINE: <2019-09-12 Thu 09:30>
   [[./Lab_HW_1_Writeup.Rmd][Here]] is the .Rmd file.
* Week 3: Diagnostics and Resampling
  <2019-09-10 Tue 09:30-10:45>
** Intro
   Tuning parameters: The settings we use for individual algorithms. Regression does not have tuning parameters, but things like random forests do. *N.B. You likely will not know which parameters are best suited for your task. The only way to find out is to run models.* Think of topic modeling as an example.
   
   Polynomial regression -- the power
   Regularization -- The penalty
   Decision Trees -- Complexity parameter (depth)
   Random Forests -- Number of trees
   
   Any other examples?
** $R$
   What we want: $_TR$ (denoting the /true/ $R$) i.e. what $R$ is in the population.

   Problem is, we only have a sample. Given this, the next best thing we can try and get an estimate of is $_TRS$, or shrunken $R$. This always falls below $_TR$, hence \textit{shrunken}.

   There are a number of ways to try and get an estimate of $_TRS$. Notably:

   - Cross-validation
   - Bootstrapping
** Why we can only get an estimate of $_TR$
   OLS is about maximizing the fit between $Y$ and $\hat{Y}$. This means being short-sighted: We don't think about other samples when creating the weights.

   This is fine and dandy because our estimates for $b_j$ are unbiased. But our model fit is biased -- We overfit our current sample.

   Big sample and small numbers of predictors: This is not a problem.

   Small sample and large number of predicts: Problem.
** An Example:
   If you add more predictors to a model, you will /always/ get a better $R^2$. However, your /adjusted/ $R^2$ may not increase. In fact, after a point, it will go down, as you are penalized for using too many predictors.

** Model Assessment vs Selection

   Model Assessment: In only running one model, or after choosing a final model, determining the most unbiased assessment of model fit, either on new data or what it would be on new data.

   Model Selection: Estimating the performance of multiple algorithms and choosing a final model among these.

** Validation Set Approach
   
   The first, and most popular way to conduct cross-validation is termed the /validation set/ approach.
   
   Randomly split the sample in half: 50% Training & 50% Test samples.

   - Training sample: Explore to your heart's desire.
   - Test sample: Treat best model parameters from training as !!! *fixed* !!! and find new model fit.

** Creating Predictions

   $$MSE_{holdout}=\frac{1}{n}\sum_{i=1}^{n}(y_{i,holdout}-\hat{f}_{training}(x_{i,holdout}))^{2}$$
   
   Important: The model is not re-estimated. We use the parameters to /create/ predictions.

** Validation Set Example

   I split a dataset into 3 parts.

   - 100 Training cases
   - 100 Validation cases
   - 2036 Test cases.

   In practice, just split 50-50 (Train-Test), don't create such a large Test dataset. This is just for demonstration.

** Performance
   - Training: $R^2 = 0.25$, $R^2_{adj} = 0.088$
   - Test: $R^2 = 0.12$
   - "Population": $R^2 = 0.19$, $R^2_{adj} = 0.18$

   With this approach, inference on the training set over-estimates the model fit, while the test sample under-estimates the fit. This is a common occurrence when starting with a small sample.

** An example
   
   You should *always* see some indication of how $R^2$ changed for your Test set and your Training set.

   If you graph this, you should see an inverted 'U' shaped curve. 

** Another Approach: k-Fold Cross Validation

   Another approach to cross-validation is termed /k-Fold/ CV, since the data is split into k number of partitions. Common to use either 5 or 10 fold. 

   Here, you split, for example, into 4/5ths training and 1/5th test. Then test how well it fits. Then shift your frame. E.g. Parts 1:4 are training, 5 is test. Then Parts 1:3 + 5 are training, 4 is test. Then Parts 1:2 + 4:5 are training, 3 is test, etc.

** Bootstrapping

   Very similar to k-fold cross-validation, bootstrapping changes one thing: samples are replaced.

   Those sample that are not selected, about 33%, are used as the holdout to "predict" on, similar to K-fold.

   Repeating this 100 times, we get an average $R^2$ of 0.056, and SE of 0.045.

   Note -- There are multiple forms of bootstrap sampling:
   1. Create bootstrap sample, then use not selected as a test
   2. Just use bootstrap sample to estimate a model

   The second form is often used to estimate standard errors.
   
** Which to Use?
   
   The validation set approach is the most commonly understood, especially in domains where Machine Learning is not well understood. Best for when you have a large initial sample and are unsure as to what method you want to use. 

   However, the validation approach results in a loss of power. If less than 1000 observations, better to use k-fold or boostrapping. If < 100, bootstrapping is better.

   *Don't take these cutoffs as absolute truth, it depends on method used and number of variables.*

   Most don't have a large enough sample size. Leading to:
   - Lack of power = underfitting

** Model Selection

   Let's say I want to compare multiple algorithms: linear regression, decision trees, and random forests.

   - I use 5-fold for model assessment
   - Just selecting the model with the lowest 5-fold Mean Standard Error (MSE) incurs a small degree of bias
   - Larger bias with more algorithms and tuning parameters used.

   This is known as /optimization bias/.
   
   This is what happens when you conflate model assessment and selection.

** Nested Cross-Validation

   Use two loops.

   Begin by performing cross validation on your training set. Then conduct cross-validation on your whole sample i.e. check your test data on your entire training data.
** Using the ~Caret~ package
*** Demo Script from Professor Jacobucci

    #+begin_src R
    library(caret)
    library(AppliedPredictiveModeling)
    library(partykit)
    set.seed(12345)
    
    # Building a sample dataset
    lat.var1 = rnorm(300)
    lat.var2 = rnorm(300)

    ## Getting an output from the predictors
    y = 0.5*lat.var1 + 0.5*lat.var2 + rnorm(300,0,1)

    ## lm() trains a linear model
    summary(lm(y ~ lat.var1 + lat.var2))

    ## Putting the data into a df
    dat1 = data.frame(y=y,x1=lat.var1,x2=lat.var2)



    # nested CV demo
    outer.repeat = 5
    method.select = rep(NA,
                        outer.repeat)
    rsq = rep(NA,
              outer.repeat)
  
    # Creating the Partitions for CV
    ## This is for the outer loop
    folds.samp1 = createFolds(dat1$y,
                              k=outer.repeat,
                              list=F) 

    # CV
    #folds.samp1 = createResample(dat1$y,times=outer.repeat,list=F) # boot -- increase number of repeats
    # Looping through the outer partitions
for(j in 1:outer.repeat){
  lm.out = train(y ~ .,
                 dat1[folds.samp1!=j,],
                 method="lm",
                 trControl=trainControl(method="cv")) # default is 10-fold
    
    
  tree.out = train(y ~ .,
                   dat1[folds.samp1!=j,],
                   method="ctree",
                   trControl=trainControl(method="cv"),
                   tuneLength=1)
    
  ## Checking the R-Squared for partitions within each outer partition. N.B. Train() is partitioning and doing CV for you behind the scene
  if (lm.out$results$Rsquared > tree.out$results$Rsquared){
      method.select[j] = "lm"
      lm.out.test = train(y ~., dat1[folds.samp1!=j,],method="lm",trControl=trainControl(method="none")) # none uses whole data
      rsq[j] = cor(dat1[folds.samp1==j,"y"],predict(lm.out.test,dat1[folds.samp1==j,]))**2
    }else{
      method.select[j] = "ctree"
      ctree.out.test = train(y ~., dat1[folds.samp1!=j,],method="ctree",trControl=trainControl(method="none"))
      rsq[j] = cor(dat1[folds.samp1==j,"y"],predict(ctree.out.test,dat1[folds.samp1==j,]))**2
    }
    
}

rsq
method.select

    #+end_src
    
** Stratified CV

   Class imbalance can cause a multitude of problems (think, what happens if you are breaking up samples and only 5% of your cases are of one category? You might get a partition without any cases for that category). One solution is Stratified CV:
   - K-fold CV with pre-allocate positive cases to ensure class balance
     
   Inconsistent results to show whether it helps. Better option is up/down sampling, which we'll cover later in the semester.

** Repeated CV

   The pre-allocation of fold partitions can impart some bias. To reduce this, repeated CV has been proposed.
   
   Entails repeated 5 or 10 fold CV multiple times (e.g., 10) to ensure adequate randomness in the resulting partitions.

** Leave-One-Out CV

   An older form of cross-validation, and maybe the simplest is leave-one-out CV (LOOCV).
   - Train on N-1, Test on 1
   - Repeat N times

   Doesn't have good of a balance of bias and variance, but will see in some small sample studies -- MRI research.

** Fit Metrics
   There are a host of different fit criteria that try and approximate bias incurred by overfitting:

   Fit = within sample fit + complexity penalty


   The most well known:
   - Bayesian Information Criterion -- penalizes for # of parameters and N
   - Akaike Information Criterion -- penalizes for # of parameters 

   Not available for most ML algorithms and only useful in a relative sense. They must compute a log likelihood ($ln(\text{likelihood})$).

** DONE Homework Assignment 2
   DEADLINE: <2019-09-19 Thu 09:30>
* Week 4: Regression
** Classification Metrics
   <2019-09-17 Tue 09:30-10:45>
*** Classification Definition

    Classification = modeling a categorical outcome
    - binary
    - multicategorical

    Binary is far more common, and most metris will generalize to multicategorical outcomes.

    Most algorithms output a predict probbability of belonging to class 1. - Clustering and others = direct assignment to class

*** Goals
    Instead of focusing on objective functions for categorical outcomes, our goal is to answer the following question: How well did our model do in predicting a categorical outcome?

    In this, we run various algorithms and create predictions. Then comparing predictions to the actual outcomes.

*** Accuracy

    $$\text{Accuracy} = \frac{1}{n}\sum_{i=1}^{n}I(y_{i}=\hat{y}_{i})$$

    What percentage of observations do we accurately classify?
    
*** Example
    We are going to try and model whether someone has endorsed depressive symptoms.
    - Use a cutscore on the BDI from the epi.bfi dataset in the psych package.

    Just use the five Big Five Inventory (BFI) scores as predictors.
    - Agreeableness, Conscientiousnous, Extraversion, Neuroticism, Openness

*** Predicted Probabilities

    [[./L_Pics/Pred_Class_Model.png]]

*** Calibration Plot

    [[./L_Pics/Calib_Plot_1.png]]

*** Confusion Matrix

    [[./L_Pics/Conf_Mat_1.png]]

    Sensitivity can be thought of as the total number of predicted positive cases divided by the /actual/ number of positive cases.

    Specificity is the counterpart of sensitivity: it is the total number of predicted negatives divided by the /actual/ number of negatives.


    |                   | Actual Outcome |              |               |
    |                   | Positive       | Negative     |               |
    |-------------------+----------------+--------------+---------------+
    | Predicted Outcome | Positive       | TP           | FP ($\alpha$) |
    |                   | Negative       | FN ($\beta$) | TN            |

*** Balanced Accuracy
    <2019-09-19 Thu 09:30-10:45>
    
    This gives you the average across both classes.

    |            | Actual |     |
    | Prediction |    Yes |  No |
    |------------+--------+-----|
    | Yes        |     14 |   8 |
    | No         |     39 | 170 |
    
    - For Yes: $14/(14+39) = 0.26$
    - For No: $170/(170+8) = 0.955$
    - Average = 0.61

      [[./L_Pics/Sens_and_Spec.png]]

*** Kappa
    We can eyeball or use other statistics to compare Accuracy to the no information rate (1-prob(class 1)), but there are other statistics that measure our ability to predict beyond chance.

    $$\text{Kappa} = \frac{O - E}{1 - E}$$

    Where $O$ is the observed Accuracy and $E$ is the expected accuracy based on the marginal totals.

    $$E =  \frac{(Predicted_{No}*Actual_{No})}{N^2} + \frac{(Predicted_{Yes}*Actual_{Yes})}{N^2}$$

    Values > 0 mean an improvement above and beyond chance.

*** Creating Cutoffs

    If you don't specify, most programs will use a cutoff of 0.5. Which, in many cases, makes sense.

    How about in our example? Our baserate is 0.22.

** Probability Metrics

*** Area Under the Curve
    The area under the receiver operating characteristic curve (AUC) can be characterized as the probability that a randomly drawn positive case has a higher probability than a randomly drawn negative case (Fawcett, 2006).

    [[./L_Pics/AUC1.png]]
    
    For this picture, the gradient is your selected cutoff rate for classifying outcomes. Sensitivity is how well it identifies positive cases. FPR is your false positive rate i.e. how frequently you erroneously predict a positive when it is negative.

    An AUC of 1 means you have perfect specificity and sensitivity i.e. you always predict correctly. An AUC of 0.5 would mean you are doing the as well as random chance would e.g. a line from one corner to the other. Less than 0.5 means you are doing /worse/ than random chance.

    Here's an example of the area under a curve for two models: Logistic and Random Forest. RF fits perfectly because there was no cross-validation.

    [[./L_Pics/AUC2.png]]

*** Problems with AUC
    Class Imbalance is a major drawback (Saito \& Rehmsmeier, 2015)
    - High imbalance will inflate the AUC

   In the suicide example before, our AUC was 0.99 in most models.
   - Even though we only found tiny effects

   Also, need a test set to get actual curve.
   - Can calculate as part of 5-fold

*** PRAUC

    Let's invert our approach: how well do we predict the positives?

    [[./L_Pics/PRAUC.png]]
    
    Unlike AUC, there is no hard and fast value to look for e.g. 0.5 is equivalent to chance. PRAUC is all relative to other models.

** Introduction to Regularization

*** Variable Selection
    Regression uses the predictors to estimate $b_j$ to maximize $R^2$. But this is within sample.

    In some scenarios, removing predictors will improve our ability to accurately estimate $_TRS^2$.

    Earlier we used CV and bootstrapping to create a better estimate of $_TRS^2$, now we want to use variable selection with CV/bootstrapping to maximize $_TRS^2$.

*** Stepwise Methods: Best Subsets Selection

    To perform best subsets selection our goal is to find the /optimal/ configuration of variables, testing /all/ possible configurations.

    Number tested equals $2^k$, where $k$ is number of variables.

    Possible configurations:
		- 5 variables = 32
		- 20 = 1,048,576
		- 100 = 1.267651e+30

    Point being? It can take a while. So we need heuristics.

*** Backward Elimination

    Backwards, start with the full model (all predictors).

    In each step, remove the variable that lowers $R^2$ the least.

    The key: What metric to use to choose a final model?

*** Information Criteria
    
    Two really popular information criteria:

    Bayesian Information Criteria (BIC; Schwarz, 1978)

    $$BIC = N*log(RSS/N) + k*log(N)$$

    Akaike Information Criteria (AIC; Akaike, 1973)

    $$AIC = N*log(RSS/N) + 2k$$

    Which is equivalent to Mallow's Cp in regression.

    The lower the better. Both have similar idea to $R^2_{adj}$, but can also be used in non-nested models (but need same people).

    E.g.

    [[./L_Pics/Backwards.png]]

*** Selecting Final Model
    Pick the simplest model with the lowest BIC e.g. Intercept, C2, C3, E3, E4, N2, O2, gender.

*** Holdout
    In using stepwise selection, do not make inferences on the sample used to select variables! P-values are no longer valid. If you have a holdout, you can test the new subsetted model on the holdout and make inferences.

    Thus, I re-run the model on the test set and can examine p-values. 

    No test set = no p-values. Sorry

*** Forward Stepwise Regression
    Start with only an intercept, select the variable that has the largest correlation with Y (positive or negative).

    Next, add variable that increase $R^2$ the most. etc... 

    You can constrain it to have requirement that improvement is significant, but what do p-values mean anymore...

    Applied to our example, we get the same results.

*** A quote on stepwise methods:
    "Stepwise variable selection has been a very popular technique for many years, but if this procedure had just been proposed as a statistical method, it would most likely be rejected because it violates every principle of statistical estimation and hypothesis testing." (Harrel, 2015, p. 67).

*** Regularization
    Stepwise methods are not necessarily deficient, it is that the procedure invites too many opportunities for abuse.

    In contrast, we will discuss a number of methods under the umbrella of a method that is termed "regularization."

    This is a family of methods designed specifically for when the number of predictors grows beyond just a few.

*** Ridge Regression

    Ordinary least squares can be represented as minimizing the residual sum of squares (RSS):

    $$RSS = argmin\Big\{\sum_{i=1}^{N}(y_{i}-b_{0}-\sum_{j=1}^{p}x_{ij}b_{j})^{2}\Big\}$$

    From this, we build a penalty term:

    $$ridge=\underbrace{RSS}_{OLS}+\underbrace{\lambda\sum_{j=1}^{p}b_{j}^{2}}_{ridge}$$

*** Lasso Regression
    Specifically to /select/ variables, the least absolute shinkage and selection operator (lasso; Tibshirani, 1996, 2011) is:

    $$lasso=\underbrace{RSS}_{OLS}+\underbrace{\lambda\sum_{j=1}^{p}|b_{j}|}_{lasso}$$

*** Alternative Equation

    $$lasso=\underbrace{RSS}_{OLS} \quad \text{subject to} \quad \sum |b_j| \le t$$

    $$ridge=\underbrace{RSS}_{OLS} \quad \text{subject to} \quad \sum b_j^2 \le t$$

    where $t$ is a tuning parameter similar to $\lambda$

*** Example 1

    [[./L_Pics/Reg_1.png]]

*** Example 2

    [[./L_Pics/Reg_2.png]]

*** Parameter Trajectory

    [[./L_Pics/Reg_3.png]]

*** Collinearity
    If I simulate two variables, $x_1$ and $x_2$, to both have $b_j$ of 1, but a correlation between them of 0.999, this is what occurs in linear regression.

    [[./L_Pics/Reg_4.png]]

*** Ridge for Collinearity
    
    |   | $\lambda$ |   x1 |   x2 |
    |---+-----------+------+------|
    | 1 |      0.00 | 1.44 | 0.15 |
    | 2 |       0.1 | 1.15 | 0.44 |
    | 3 |       0.2 | 1.04 | 0.55 |
    | 4 |       0.3 | 0.98 | 0.60 |
    | 5 |       1.0 | 0.86 | 0.72 |
    
    By penalizing both coefficients, it /shrinks/ the inflated coefficients and inflates the suppressed varaibles.

    In general, no easy remedy for collinearity.

*** Main Points

    Ridge regression:
    - Handles collinearity.

    Lasso regresson: 
    - Performs variable selection.

    As a way to combine these, Zou and Hastie (2005) proposed the /elastic net/ (enet) regularization. Through the use of a mixing parameter, $\alpha$, the elastic net combines both ridge and lasso regularization

    $$enet= \underbrace{RSS}_{OLS} + \underbrace{(1-\alpha)\lambda\sum_{j=1}^{p}\beta_{j}^{2}}_{ridge} + \underbrace{\alpha\lambda\sum_{j=1}^{p}|\beta_{j}|}_{lasso}$$

*** Elastic Net Note

    One drawback: generally not as sparse as the lasso
    
    Also, more computationally intensive.
    - Could just test value of 0.5
    - Or 0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1

    Typically worth doing manually with ~train()~.
    - glmnet won't test vector
      
*** Example Script from Prof. Jacobucci

    #+begin_src R
#ecls.1 = read.table('C:/Users/rjacobuc/Documents/GitHub/edm_book/ch5_trees/ecls_DM.dat', na='.');
#ecls.1 = read.table('/Users/rjacobuc/Documents/GitHub/edm_book/ch5_trees/scripts/ecls_DM.dat', na='.');
#ecls.1 = read.table('C:/Users/rjacobuc/Documents/GitHub/edm_book/ch5_trees/scripts/ecls_DM.dat', na='.');

#library(synthpop)

#ecls_syn <- syn(ecls.1)$syn
#write.csv(ecls_syn,"G:/My Drive/PSY-ML-Fall19/4.Scripts/ecls_syn.csv")

ecls_syn <- read.csv(file.choose())

names(ecls_syn) = c('gender','kage',
                  'k_read_irt','k_read1','k_read2','k_read3','k_read4',	
                  'k_print','k_read_tht',
                  'k_math_irt','k_math1','k_math2','k_math3','k_math4',
                  'k_math_tht',
                  'k_gk_irt','k_gk_tht',
                  'f_mtr','g_mtr',
                  'P1LEARN','P1CONTRO','P1SOCIAL','P1SADLON','P1IMPULS',	
                  'ars_lit','ars_mth','ars_gk',
                  'T1LEARN','T1CONTRO','T1INTERP','T1EXTERN','T1INTERN',	
                  'height','weight','bmi',
                  'hisp','na_amer','asian','black','pac_isl','white','m_race',
                  'ses_c','ses_cat','poor','income',
                  'g8_read','g8_read_tht','g8_math','g8_math_tht',
                  'g8_sci','g8_sci_tht')

# subset of variables
x.vars = c('k_math_irt','k_gk_irt',"income","bmi","gender")
set.seed(1)
x.noise <- matrix( rnorm(nrow(ecls_syn)*20,mean=0,sd=1), nrow(ecls_syn), 20) 
y.vars = 'g8_sci'
ecls_syn$gender = ecls_syn$gender - 1
XX <- data.matrix(cbind(ecls_syn[,x.vars],x.noise))
XX.std <- data.matrix(cbind(scale(ecls_syn[,x.vars]),x.noise))
YY <- as.numeric(scale(ecls_syn[,y.vars]))



# linear regression
dat.comb <- data.frame(YY,XX.std)
lm.out <- 






library(glmnet)
# ?glmnet
# return in original scale
lasso.out <- glmnet(XX,YY,family="gaussian",alpha=1)
plot(lasso.out)

lasso.cv <- cv.glmnet(XX,YY,family="gaussian",alpha=1)
plot(lasso.cv,size=2)

round(coef(lasso.cv,lasso.cv$lambda.1se),3)
round(coef(lasso.cv,lasso.cv$lambda.min),3)


relax.out <- lm(scale(g8_sci) ~ scale(income) + scale(k_math_irt) + scale(k_gk_irt),ecls_syn)
summary(relax.out)
round(coef(relax.out),3)

# ridge

ridge.out <- glmnet(XX,YY,family="gaussian",alpha=0)
plot(ridge.out)

ridge.cv <- cv.glmnet(XX,YY,family="gaussian",alpha=0)
plot(ridge.cv)
round(coef(ridge.cv,ridge.cv$lambda.1se),3)


#lasso.std <- glmnet(XX.std,YY,family="gaussian",alpha=1,intercept=F,standardize=F)
#plot(lasso.std)

# penalized package 
#library(penalized)
#fit1 <- penalized(YY, penalized=XX, unpenalized=~0, standardize=TRUE) 
#round(coefficients(fit1),3)


# lasso p-values

library(covTest) # have to install from cran arxiv https://cran.r-project.org/src/contrib/Archive/covTest/
a=lars.en(XX,YY,lambda2=0)
cov.out = covTest(a,XX,YY)
cov.out$results[1:4,1] # get predictor numbers

head(XX[,c(2,1,3,5)])




# hierarchical lasso
set.seed(1)
ids <- sample(1:nrow(XX.std),nrow(XX.std)*.5)
XX.train <- XX.std[ids,]
XX.test <- XX.std[-ids,]
YY.train <- YY[ids]
YY.test <- YY[-ids]

library(hierNet)

out = hierNet.path(XX.train,YY.train,strong=TRUE)
out.cv = hierNet.cv(out,XX.train,YY.train)

plot(out.cv)
out.cv

out$th[,,11] # interactions and quadratic effects
out$bp[,11] # positive main effects
out$bn[,11] # negative main effects

# test if weak identifies others.
out.weak = hierNet.path(XX.train,YY.train,strong=FALSE)
out.cv.weak = hierNet.cv(out.weak,XX.train,YY.train)
plot(out.cv.weak)
out.cv.weak

out.weak$th[,,11] # interactions
out.weak$bp[,11]
out.weak$bn[,11]


# re-run model

dat.comb <- data.frame(XX.test,YY.test)
colnames(dat.comb)[1:5] <- c("math","knowledge","income","bmi","gender")
colnames(dat.comb)[26] <- "science"

lm.int <- lm(science ~ math + knowledge + income + bmi + gender + 
               I(math^2) + I(knowledge^2) + I(income^2) + I(bmi^2) +
               math*knowledge + knowledge*income,
             dat.comb)
summary(lm.int)
stargazer::stargazer(lm.int,column.sep.width = "1pt",single.row = TRUE)




# try stability selection

# stability
library(stabs)
stab.out11 <- stabsel(XX,YY,cutoff=0.75,PFER=1)
stab.out11
    #+end_src

** Regularization and Bias-Variance
*** APE Exposure Data
    [[./L_Pics/APE_Data.png]]
*** 20th Order Polynomial
    [[./L_Pics/Reg_5.png]]
*** Random Forests
    [[./L_Pics/Reg_6.png]]
*** Bias vs. Variance
    [[./L_Pics/Bias_v_Var.png]]
*** Fit Decomposition
    $$MSE = Bias^{2} + Variance$$

    $$\mathop{\mathbb{E}} \left[ (y - \hat{f}(x))^2  \right] = \left(Bias[\hat{f}(x)] \right)^2 + Var[\hat{f}(x)] + \sigma^2$$

    $$Bias[\hat{f}(x)] = \mathop{\mathbb{E}}[\hat{f}(x)] - \mathop{\mathbb{E}}[f(x)]$$

    $$Variance[\hat{f}(x)] = \mathop{\mathbb{E}}[\hat{f}(x)^2] - \mathop{\mathbb{E}}[f(x)]^2$$

    Its about decomposing the expected error on an unseen sample (population).

    - Overly simple model = High Bias
    - Overly complex model = High variance
*** Decomposing

    [[./L_Pics/Decomp.png]]
*** Model Fit
    [[./L_Pics/Model_Fit.png]]
*** OLS vs. Regularization

    Simulate data acording to a linear regression -- run model on sample size of 40.
    [[./L_Pics/OLS_v_Reg.png]]
*** Sample Size

    The Bias-Variance Tradeoff is typically discussed with a fixed N.

    As sample size decreases, variance of fixed model will increase.
    - Less information to model
    - Conclude simpler model with fit better
*** Regularization and Sample Size

    [[./L_Pics/Reg_and_Samp.png]]

*** Sample Size Continued

    Example evaluated on test set.
    - Why more data can beat better algorithms

    [[./L_Pics/Samp_Size.png]]
    
*** Conceptual Quiz

    What are the bias and variance properties of the following algorithms. Detail what data generating scenarios for which this holds:

    - 20th degree polynomial vs linear regression
    - Linear Regression vs. ridge regression

*** Big Picture Takeaways
    Why are we bothering with this?
    - We can't see bias or variance, so we need to consider this before throwing algorithms at data. Otherwise, you have a recipe for poor prediction/capturing of trends.

    When to prefer bias?
    - When you have few observations e.g. less than 1000
    - When you want to eliminate variables

    When to prefer variance?
    - If you have a lot of observations

* Week 5: Regression (cont.)
** Regularization: Advanced Methods
   <2019-10-01 Tue 09:30-10:45>

*** Lasso P-values

    Simply put: researchers like p-values. With the lasso, it is not recommended to do:
    - bootstrap within the algorithm to get p-values
    - assess p-values in a follow-up step

    Lockhart, Taylor, Tibshirani, and Tibshirani (2014) proposed a new distribution for the test statistic.
    - implemented in ~selectiveInference~ package (this was the ~covTest~ package previously)
      
    In this, use p-value for selection, not whether non-zero.

*** Stability - Variable Selection and Stability
    Variable slection is inherently unstable. Two factors that play a large role:
    1. Sample size
    2. Signal in predictors
       
    If you have small effects among your predictors, you are going to have problems with stability. More specifically, sometimes the lasso will select a variable as being non-zero, and sometimes it will not. This is not unique to the lasso; the same problem exists for other methods.

*** Stability Selection

    As opposed to just bootstrapping the variable selection process, Stability Selection (Meinshausen & Buhlmann, 2010) developed a more formal process. It involves:
    - Taking at least 100 bootstrap samples (or subsamples) for each penalty
    - Selecting a subset of the penalty range to asses selection probabilities for each predictor
    - Range of penalites is based on eseeting a false positive error rate
      
    N.B. you will set your own threshold. 0.8 or 0.9 are generally pretty good standards.

    [[./L_Pics/Ad_Reg_2.png]]

    [[./L_Pics/Ad_Reg_1.png]]

*** Moderation Analysis
    Consider a model
    $$Y_i = b_0 + b_XX_{i} + b_ZZ_{i} + \epsilon_i$$ where $Z$ is a potential moderator. To determine whether  $Z$ moderates the effect of $X$ on $Y$ we can fit $$Y_i = b_0 + b_XX_{i} + b_ZZ_{i} + b_{XZ}X_iZ_{i} + \epsilon_i$$ and test the hypothesis that $$b_{XZ}=0$$

    If $$b_{XZ} \neq 0$$, we can claim that $Z$ moderates the relationship between $X$ and $Y$.

    Equivalently, we can say that there is an /interaction/ between $X$ and $Y$ or that the relationship between $X$ and $Y$ is conditional on $Z$.

*** Parameter Interpretation

    $$Y_i = b_0 + b_X(X_{i}-\bar{X}) + b_Z(Z_{i}-\bar{Z}) + b_{XZ}(X_{i}-\bar{X})(Z_{i}-\bar{Z}) + \epsilon_i$$

    [[./L_Pics/Ad_Reg_3.png]]

    [[./L_Pics/Ad_Reg_4.png]]

*** Searching for Interactions

    A number of methods have been developed for high-dimensional selection of interactions: (Bien, Taylor, & Tibshirani, 2013; Choi,Li, & Zhu, 2010; Haris, Witten, & Simon, 2016).

    $$Y=\beta_{0}+\sum_{j}\beta_{j}X_{j}+.5\sum_{j\neq k}\Theta_{jk}X_{j}X_{k}+\epsilon$$

    $\Theta_{jk}$ is a matrix assessing every possible interaction between variables. 

    In R, there are the ~hierNet~ and ~FAMILY~ packages. Both are slow. 

    Have to choose an assumption to follow:
    Strong hierarchy -- Only test interactions if both variables have simple effects.
    Weak hierarchy -- Have to have one non-zero simple effect.

    N.B. Quadratic effects can be masked by interactions and vice versa.

*** Example Results

    [[./L_Pics/Ad_Reg_5.png]]

*** Hierarchical Testing

    When we have /sets/ of predictors, we are often interested in knowing whether the whole set is related to the outcome.

    Note that this should be technically called /Hierarchical testing/, /analysis/, or /entry/ to differentiate from Hierarchical/Multilvel/Mixed models.

    This form of testing represents a /confirmatory/ analysis. The steps are determine /a priori/. 

    What variables and in what steps/order is determined theoretically. This method can be tailored to fit what is of theoretical interest to you.

*** Example Hypothesis

    I hypothesize that conscientiousness will predict agreeableness even after controlling for demographic variables. 

    Model 1: $ agree = b_0 + A*demo$
    Model 2: $agree = b_0 + A*demo + B*conscientiousness$

*** Additional Example

    I wish to test the following sequence

    Model 1: $agree = b_0 + A*demo$
    Model 2: $agree = b_0 + A*demo + B*consc$
    Model 3: $agree = b_0 + A*demo + B*consc + C*open$
    Model 4: $agree = b_0 + A*demo + B*con + C*open + D*neur$

    Which produces the following relationship:

    $R^2(Model 4) = \Delta R^2(Model 4) + \Delta R^2(Model 3) + \Delta R^2(Model 2) + R^2(Model 1)$
    
*** Selecting Among Groups

    For a scenario where the numbers of variables preclude strong theoretical specification, Yuan and Lin (2007) proposed the group lasso

    $$grplasso=argmin\Big\{\sum_{i=1}^{N}(y_{i}-\beta_{0}-\sum_{j=1}^{p}x_{ij}\beta_{j})^{2}+\lambda\sum_{g=1}^{G}\sqrt{p_{g}}\|\beta_{g}\|_{2}\Big\}$$

    where $p_{g}$ accounts for the varying sizes of groups and $\beta_{g}$ is the vector of all $\beta$ coefficients in group $g$. In the group lasso, the lasso penalty acts at the group level, where instead of individual parameters are set to zero, entire groups of parameters can be pushed to zero.
** DONE Homework Assignment 3
   DEADLINE: <2019-10-03 Thu 09:30>
* Week 6: Trees
** Decision Trees
   <2019-10-03 Thu 09:30-10:45>
*** Intro
    Decision Trees (DT) were first introduced in the 1960's by Morgan and Sonquist (1963). However, the use of the method did not capture the attention of the statistics community until 20 years later with the work of Breiman, Friedman, Olshen, and Stone (1984).

    Also referred to as:
    - Recursive partitioning
    - CART
    - Classification and regression trees
*** Algorithm Overview
    First, divide the predictor space into all possible unique values. Across these unique predictor values, the algorithm iteratively partitions the dataset to determine an improvement in model fit. This involves placing each observation into one of two groups, of which, each group member receives the same predicted response.

    Given /m/ unique sorted values, this involves testing /m-1/ unique splits. For nominal variables, as there is no ordering to the values, all possible groupings are tested. If there are /m/ categories, this involves testing 2^{m-1}-1 dichotomous splits.

    For continuous outcomes, this is generally mean squared error. For categorical, it can be accuracy or two alternatives such as entropy or the gini index.

    It then checks each new group and sees if it can improve the fit by splitting the data further.
*** Example Tree
    [[./L_Pics/D_Tree_1.png]]
    
    Each split is created by testing each predictor variable and checking every internal partition within that variable to improve the fit. In other words, splitting the "worthless" variable at 0.5 is the /best/ way of reducing the error for the first data split.
*** Terminology
    [[./L_Pics/D_Tree_2.png]]
*** Prediction Line (or, relationship between 1 predictor and 1 outcome)
    [[./L_Pics/D_Tree_3.png]]
*** Prediction Surfaces (or, 2 predictors for 1 outcome)
    [[./L_Pics/D_Tree_4.png]]
*** Interactions
    [[./L_Pics/D_Tree_5.png]]
** Decision Trees II
   <2019-10-08 Tue 09:30-10:45>
*** Algorithms
    The term DT often refers to the use of classification and regression trees. It denotes the overarching methodology. Classification and Regression Trees (CART; Breiman et al., 1984) refers to a specific algorithm that falls under the umbrella of DT.

    Two different packages implement variants
    - rpart
    - tree

    Often time this tree structure fits the data too well (overfits), meaning that parts of the tree structure are unlikely to generalize well to alternative samples. One strategy to overcome this prpoensity is to _prune_ the initial tree back to a smaller size.
*** Pruning
    In choosing a final model, it is common to build a large tree and then prune back the tree to select a sub-tree i.e. a smaller version the tree that minimizes the cross-validation error. This is done in order to prevent "missing" an important additional split (see Breiman, Friedman, Olshen, and Stone, 1984).

    In practice, pruning involves testing multiple tuning parameters that control the size of the tree, using CV to select among these. Conversely, we can create a tree without attempting to control the size of it, the proceed to _prune_ back the leaves to create a series of submodels/subtrees. Both typically result in the same tree.

*** Cost Complexity

    The cost complexity measure is:

    $$R_{\alpha}(T_{p})=R(T_{p})+\alpha s_{p}$$

    where $R(T_{p})$ is the error, and $s_{p}$is the number of leaves (terminal nodes) for tree $T_{p}$(Breiman et al., 1984). When $\alpha$ is zero, then we have the original tree structure.

    What does this look similar to? Regularization. This is the same thing but for decision trees.

*** Example Results
    | CP | Splits | Sample Error | Avg. CV Error | CV Error Std. |
    |----+--------+--------------+---------------+---------------|
    |  1 |      0 |            1 |             1 |          0.03 |
    |  2 |      1 |         0.83 |          0.83 |          0.03 |
    |  3 |      3 |         0.77 |          0.84 |          0.03 |
    |  4 |      5 |         0.74 |          0.82 |          0.03 |
    |  5 |      6 |         0.73 |          0.83 |          0.03 |

    Table 1: Table of complexity parameters and fit from Example 1.
    
    Note: there are big benefits to having small trees e.g. stability. But... readers may not trust your results. They may assume that they are not receiving all of the information. It's up to you to decide what to report.

*** Example cont.

    To demonstrate this process we can use the ECLS-K data. This time, we predict reading achievement in grade 8. The sample was split into both a train and test set. For each tree, we recorded the fit on the train set, the average fit across the 10 folds, and on the test set.

    [[./L_Pics/D_Tree_6.png]]

*** Conditional Inference Trees

    Conditional Inference Trees (Ctree; Hothorn , hornik, & Zeileis, 2006) are based on a general theory of permutation tests, performing a hypothesis test at each node resulting in a p-value criterion to test whether the tree should stop or keep growing. Using a perutation test to calculate a p-value entails comparing a split on the original sample to using that same split on randomly shuffled response values (e.g. swapping observation 1's and 2's response).

    This overcomes a well known problem with CART (Kin & Loh, 2001). It does not control for the number of response options.

*** Partykit package

    The algorithm is iimplemented as the ~ctree~ function in the ~partykit~ package (Hothorn & Zeileis, 2015). The default for the p-value criterion is 0.05 (expressed as 0.95, or 1-p).

    Can use in ~train()~ with ~method="ctree"~ . Using the default tuning parameters (3) is pretty reasonable, while when using ~method = "rpart"~, its best to test 20-30 or so.

*** Stability
    
    Example:

    To make the concept more concrete, we will use the ~Holzinger-Swineford~ dataset from the ~lavaan~ package (Rosseel, 2012). With this dataset, we created three trees using the ~rpart~ package, one on the first 100 respondents, one of the second 300 respondents, and finally on the entire sample (N=301). The resulting tree structures for each are randomly different but they seem to reduce our error by commesurate levels.

    [[./L_Pics/D_Tree_7.png]]

    Adding backward selection to choose which variables should be in the model (usiing the Akaike Information Criterion to choose a final model), we get additional uncertainty. In the first sample, variables x4 and x5 are chose in the second sample x3, x4, and x6, while in the full sample x2 and x4.

    Unsurprisingly, small trees are more stable. However, this comes at the cost of prediction quality and inference. Similar concept to bias and variance, except here there is high variability to the interpretation when bias is low.

*** Measuring Stability

    If inference is of high priority, it is worth quantifying how unstable a tree is.

    The easiest solution is using 100 bootstrap samples to see how often the same tree is created.

    The is made relatively easy with the ~dtree~ package.

    |       | nodes       | nvar | nsplits | RMSE CV | $R^2 CV$ |
    |-------+-------------+------+---------+---------+----------|
    | lm    | -           |    - |       - |   11.75 |     0.37 |
    | rpart | 15.42 (4.4) | 2.00 |   14.42 |   11.51 |     0.39 |
    | ctree | 22.67 (9.4) | 2.00 |   21.67 |   11.48 |     0.40 |

*** Additional Information: Surrogate Splits

    Two options for handling missingness in trees:
    - Multiple imputation
    - Surrogate splits

    Surrogate splits work in the following way: after a primary split is found for a given node, surrogate splits can be found by re-applying the partitioning algorithm to predict the binary primary split. For example, if the primary split is on education between less than or equal to 12 years and greater than 12, then this new binary variable becomes the outcome, with the remaining variables used as predictors.

*** Variable Importance

    Similar to that of linear regression, having high correlations among covariates presents problems for DT. In the tree building algorithm, at a given split two gollinear variables may produce almost idential improvements in fit, but only one can be chosen for a given split. This is analogous to the idea of masking, but in the case of DT, it results in one of the variables either not being split on at all, or lower in the tree. To quanitfy how influential a variable is in predicting the outcome variable, one can also calculate the varaible importance metric for a given tree.

    Add collinear variable messes up the tree, but variable importance were as follows: 39 for math, 33 for knowledge, and 28 for the simulated math score.

*** Summary

**** Advantages:

     - Are robust to outliers - single misclassifications don't largely influence the splitting.
     - Perform variable selection
     - Genrally result in easy to interpret tree structures
     - Automatically include interaction 

**** Disadvantages:
     - Instability
     - Collinearity presents problems
     - Relatively lower predictive performance

*** DONE Homework Assignment 4

** Tree Recap
*** Surrogate Splits
    What are they?
    - First, we produce a split for a node. What happens if our predictor is missing values? We can check all variables to see what variable is the best predictor of the original predictor. For example, say our original variable is ~MATH~. However, we are missing values. But of our predictors, ~IQ~ serves as the best predictor of ~MATH~. So for our missing ~MATH~ values, we check how we split our observations if we had split based on ~IQ~ and then put individuals in each category into the original groups for ~MATH~ i.e. if our outccomes are 0 and 1, anyone predicted to be a 0 for ~IQ~ is put into the predicted 0 node for ~MATH~.
* Week 7: Ensembles
** Bagging
   <2019-10-10 Thu 09:30-10:45>
*** Downfalls to Decision Trees

    We've already discussed some of the drawbacks to decision trees, namely:
    - Poor predictive performance
    - Instability in structure
    - Bias in variable selection (CART)

    Here, we are going to discuss one of the main algorithms proposed to overcome these problems: random forests. Prior to discussing RF, we will look at bagging, a precursor.

*** Bagging Overview

    One of the first methdos proposed to overcome the limitations of decision trees is bootstrap aggregating, or bagging (Breiman, 1996).

    The algorithm proceeds in two steps. First, a bootstrap sample is taken. Then for each bootstrap sample, fit a tree model using all predictors. Given that each bootstrap sample is different, each tree will tend to have a unique structure as well, capitalizing in the instability of decision trees, manifested in either the variables used, splitting values, or tree size.

*** Bagging Predictions

   $$\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^{B}\hat{f}^{*b}(x)$$ 

   ... where our predictions $\hat{f}_{bag}(x)$ are take as the average predictions across each tree $\hat{f}^{*b}(x)$ for B number of trees created, with B being approximately 500. In the case of a categorical outcome, we can use majority vote across the trees.

   By averaging the prediction across 100s of trees, we reduce the variance.

*** Example

    Using the PHE Exposure data, we start with a single tree.

    1 Tree:
    [[./L_Pics/RF_1.png]]
   
    5 Bagged Trees:
    [[./L_Pics/RF_2.png]]
    
    2000 Bagged Trees:
    [[./L_Pics/RF_3.png]]

*** Testing Different Numbers of Trees

    Using the same example, we can see how our predictive performance changes across an increasing number of trees.

    [[./L_Pics/RF_4.png]]
    
*** Drawbacks to Bagging

    When using the same set of predictors to create each of the trees, many of the trees will be overly similar (or the same), resulting in a limited search of the prediction space. Bagging reduces the variance of individual trees, but the bias is approximately the same since the same trees are used. Put more concisely, the trees from bagging typically have high correlations.

    Random forests was proposed ot keep the low variance of bagging, but further reduce the bias by de-correlating the trees. This is achieved by randomly selecting a subset of the variables for the creation of each tree.

** Random Forests
*** Algorithm
    
    *for* b = 1 to B *do*
    1. Draw a boostrap sample of size N from training data
    2. Grow a decision tree $T_b$ on the bootstrap sample using $m$ variables
    *end*
    *Result:* Output the ensemples of trees $\{T_B\}^B_1$
    
    To create predictions:

    $$\hat{f}_{rf}^{B}(x)=\frac{1}{B}\sum_{b=1}^{B}T_{b}(x)$$
    
    Note that some versions of random forests use subsamples to create trees, not boostrap samples.

*** ~mtry~

    ~mtry~, or the number of randomly selected variables for creating each tree, is the main tuning parameter in random forests. You can vary the number of trees, but it typically doesn't matter for prediction (different for variable importance).

    Default values in most programs:
    - Classification is $sqrt{p}$
    - Regression is $p/3$

*** Out of Bag Samples

    When using bootstrap samples, can use of the out of bag (OOB) samples to create samples. Then, average predictions for each observation just in those trees in which they were used (e.g., (1-.632)*B number of trees).

    Using OOB prediction performance approximates the use of k-fold pretty well. However, it is more common to just use k-fold.

*** Compared to Bagging

    [[./L_Pics/RF_5.png]]
    
    Random forests are, by nature, much more likely to overfit your data.

* Week 8: Ensembles (cont.) and Project Proposals
** Random Forests (cont.)
   <2019-10-17 Thu 09:30-10:45>
*** Variable Importance
    Why not just univariate assessment?
    
    Assessing the importance of all predictors in the same model, as opposed to using univariate models, can capture the multivariable \textbf{interactions}.
    
    Particularly when predictors are correlated, the multivariable assessment can result in different inference than univariate.

    Multivariable assessments can also remove spurious effects in a univariate model -- a variable associated with the outcome only because of its relation to a different predictor.

*** Variable Importance in a Single Tree
    
    From Breiman et al. (1984)

    $$I_{l}^{2}(T) = \sum_{t=1}^{J-1}\hat{i}_{t}^{2}I(v(t)=l)$$

    ... for each predictor $X_l$, $J-1$ internal nodes, each node $t$, and $\hat{i}_{t}^{2}$ is a measure of error improvement.

    Taking the square root gives an importance value for each predictor. 

    What are the issues with this?
    - This only gives non-zero values for variables that were used in the tree. If it wasn't used in the tree, you don't have an accurate measurement of importance.
    - ~rpart~ gets around this by using surrogate splits to approximate this.

*** Variable Importance Across Trees
    In both boosting and random forests, can alter this to:

    $$I_{l}^{2} = \frac{1}{M}\sum_{m=1}^{M}I^{2}_{l}(T_m)$$

    ...where M is the number of trees.

    Key point: this is relative, so common to assign largest value to 100 then scales each accordingly.

*** Permutation Tests

    From Strobl et al. (2008): "that a random permutation of the values of the predictor variable is supposed to mimic the absence of the variable from the model."

    This form of VI was proposed in Breiman (2001): Using the out of bag samples (OOB), for the Mth tree grown, pass the OOB samples down the tree, and record the prediction performance. Then, for the $lth$ variable, permute the values and pass down the new OOB sample data, and record the prediction performance. In contrast to CTree permutation, this is change the actual values of X.

*** Problems with RF VI

    The way of permuting X values in RF can be referred to as a marginal approach. This has been found to bias VI values in favor of predictors that are correlated. As a result, Strobl et al. (2008) developed a conditional approach. 

    For the permutation, add this: "For all variables Z to be conditioned on: Extract the cutpoints that split this variable in the current tree and create a grid by means of bisecting the sample space in each cutpoint."

    Then calculate OOB VI of X *within* each point in the grid.

*** Random Forest Example

    The randomForest package, method="rf", by default uses the permutation test on the OOB approach. Using the ECLS-K data and 100 trees:

		|           |     RF | Rpart |
		|-----------+--------+-------|
		| math      |  17.29 |    13 |
		| knowledge |  28.14 |    13 |
		| weight    |   1.37 |     0 |
		| height    |   0.00 |     0 |
		| income    |  10.95 |     6 |
		| ses       |  10.94 |     8 |
		| read      | 100.00 |    61 |

*** cforest example

    From the partykit package:

    		|           |    CF |
    		|-----------+-------|
    		| math      |  8.81 |
    		| knowledge |  9.98 |
    		| weight    |  0.17 |
    		| height    |  0.27 |
    		| income    |  2.09 |
    		| ses       |  2.46 |
    		| read      | 97.08 |

*** Number of Trees

    For prediction, as long as the number of predictors is small, the number of trees typically doesn't matter. 

    However, for calculating VI, a higher number may be better so there isn't instability to VI.

*** Visualizing the Effects of X

    The idea behind partial dependence plots is to visualize the effect of a variable in a "black box" type algorithm. 

    Broadly speaking, this is examining the effect of $X_1$ on $Y$, after accounting for the average effects of $X_C$. This approach, developed by Friedman (2001) allows for the visualization of interactions and nonlinear effects.

    These are descriptive approximations, not "true" derived relationships.

*** Reading

    Using Random Forests and reading as a predictor:

    [[./L_Pics/RF_6.png]]

*** Math

    Using Random Forests and math as a predictor:

    [[./L_Pics/RF_7.png]]

*** Using Two Predictors

    [[./L_Pics/RF_8.png]]

*** Another Way

    [[./L_Pics/RF_9.png]]

* Week 9: No Class (Fall Break)
* Week 10: Ensembles (cont.)
** Boosting
   <2019-10-29 Tue 09:30-10:45>
*** Introduction
   Similar to random forests, boosting is a method that combines the
   use of hundreds (or thousands) individual decision trees in an attempt
   to overcome some of the problems with single trees. However, in contrast
   to random forests, where each individual tree is created independently
   of every other, boosting creates a sequence of trees based on re-weighted
   forms of the dataset. 

   Developed originally for classification in the
   form of the AdaBoost algorithm (Freund & Schapire, 1995; 1996; 1997),
   the idea behind the algorithm has been extended to many other statistical
   problems (see Buhlmann & Hothorn, 2007 for more detail). Most notably,
   Friedman, Hastie, and Tibshirani (2000), as well as Friedman (2001),
   developed a highly adaptable algorithms that can be applied to a host
   of different loss functions, as well as extended to multivariate outcomes.
*** Algorithm

    The basic idea of this algorithmic framework, originally termed “gradient boosting machines” by Friedman (2001), now more frequently just called “boosting,” is to fit an additive combination of what is termed a “weak learner,” most often a decision tree (weak in the sense that its predictive performance is usually substandard), to the gradient (residuals) of the prior combination of weak learners. More formally, the algorithm is described by:

    [[./L_Pics/Boosting_1.png]]

*** Learning Slowly
    
    This algorithm creates a large sequence of try by continually fitting trees to each current vector of residuals. By shrinking the influence of each tree, and thus how much the residuals are updated, overfitting can be prevented, better known as the process of learning slowly. This is facilitated by the use of the shrinkage parameter (learning rate; $\lambda$), which is usually treating as a tuning parameter, with typically values ranging from 0.001 to 0.1. The smaller the value of $\lambda$, the more trees that could be needed to adequatly fit the data

*** Training Error

    [[./L_Pics/Boosting_2.png]]

*** CV Error

    [[./L_Pics/Boosting_3.png]]
    
*** Number of Trees

    Partially depends on sample size -- smaller = less trees.

    Interaction between learning rate and number of trees -- why testing grid is important. 

    Also, more predictors, need more trees. Somewhere around 500-1000 is typical.

*** Tree Complexity

    The tree depth (complexity) should reflect the order of possible interaction in the data. 

    In social and behavioral research, doubtful to have greater than order 3, however, may be worth testing 5. 

    This means worth testing a depth of: 1 (stump; just main effects; nonlinear effects), 2 can capture interactions, and 3 (higher-order interactions). 

*** Deductive Data Mining

    We developed a method termed deductive data mining: https://osf.io/preprints/bmwah/

    Idea is similar to the Elith et al. paper, use the information from interaction depth to follow up with manual interactions and quadratic effects. 

*** Algorithms
    
    Traditional gradient boosting, implemented in the gbm package, is by far the most popular algorithm. In caret, method = "gbm".  

    A newer algorithm, termed extreme gradient boosting (Xgboost; Chen & Guestrin, 2016) typically has better predictive performance, but the variable importance calculation is a little weird. Implemented in caret as method="xgbTree". 

*** ~gbm~ Package

    gbm has four tuning parameters:
    - number of trees (n.trees)
    - max tree depth (interaction.depth)
    - shrinkage (shrinkage)
    - min. terminal node size (n.minobsinnode)

    In comparison to random forests, boosting takes more tuning. It is worth doing a first run, and then based on results, alter tuning values. 

    Note -- this will also take longer than RF. 

*** Tuning Example

    [[./L_Pics/Boosting_4.png]]

*** Partial Dependence Plots

    Using the ~dismo~ package has additional functionality. One is auto partial dependence plots:

    [[./L_Pics/Boosting_5.png]]

    [[./L_Pics/Boosting_6.png]]

*** Summary

    Gradient boosting with trees is one of the most popular machine learning algorithms. A number of details worth noting when deciding between RF and boosting:
    - boosting often takes longer
    - boosting requires testing more tuning parameters
    - boosting allows for evaluating interactions
    - both can utilize partial dependence plots
    - random forests has been evaluated further with variable importance

** N.B. Absent for lecture on <2019-11-01 Fri 09:30-10:45>. Long week, tired, Winston sick.
* Week 11: Nonlinear methods
  
** Mixture Models
   <2019-11-12 Tue 09:30-10:45>

*** Posterior Probability of Class Membership

    - Each individual has a posterior probability of belonging to each class
    - Ideally, these probabilities are strong (near 1) for a single class and weak for other classes
    - Accounts for uncertainty in class assignment
      - Important when comparing classes on various outcomes and determinants

*** Evaluating Model Fit

    - Statistical Model fit
      - Log Likelihood
        - If we didn't penalize our models for complexity, this would always select the most complex one
      - Penalizing for Complexity:
        - Akaike Information Criteria
        - Bayesian Information Criteria
    - Model Usefulness
      - Substantive Interpretation
      - Classification Quality: how well are we able to classify both groups?
        - Classification Table
        - Entropy

*** Practical Issues
    
    There is no minimum sample size.
    - Sample size is related to the number and type of classes you can identify

    Sample size and the outcomes both influence the type and number of classes. Within-class model and between-class contraints determines the type and number of classes

*** Estimation Issues
    
    - Convergence
      - Log-Likelihood should increase smoothly; absolute and relative changes should go to zero; class counts should be stable
    - Convergence problems
      - May need better starting values
      - May indicate nonidentification

    In short, do not be suprised if you try to run a mixture model and you get problems e.g. "Couldn't find a convergent model."

*** Convergence Issues

    You need to have multiple starting points for parameter estimation. Otherwise, you may reach a local optimum but not a global optimum.
    
    More dimensions means you will likely need to test more starting values. If you get a collection of inconsistent log-likelihoods, your surface is likely poorly defined. You want a consistent global optimum.
    
    [[./L_Pics/Mixtures_pt2.png]]

*** An Example:
    
**** Simulated Data:

     Let's say we have data with 2 variables pulled from 3 groups.
     
     - data.1 = data.frame(grp = 1, x = rnorm(100, mean = -2, sd = 1), y = rnorm(100, mean =  2, sd = 1))
     - data.2 = data.frame(grp = 2, x = rnorm(200, mean =  2, sd = 1), y = rnorm(200, mean = -2, sd = 1))
     - data.3 = data.frame(grp = 3, x = rnorm(125, mean =  2, sd = 1), y = rnorm(125, mean =  2, sd = 1))
       
     [[./L_Pics/Mixtures_pt2_2.png]]

**** ~mclust~ models

     https://cran.r-project.org/web/packages/mclust/vignettes/mclust.html
     - multivariate mixture
     - "EII" = spherical, equal volume
     - "VII" = spherical, unequal volume
     - "EEI" = diagonal, equal volume and shape
     - "VEI" = diagonal, varying volume, equal shape
     - "EVI" = diagonal, equal volume, varying shape
     - "VVI" = diagonal, varying volume and shape
     - "EEE" = ellipsoidal, equal volume, shape, and orientation
     - "EVE" = ellipsoidal, equal volume and orientation (*)
     - "VEE" = ellipsoidal, equal shape and orientation (*)
     - "VVE" = ellipsoidal, equal orientation (*)
     - "EEV" = ellipsoidal, equal volume and equal shape
     - "VEV" = ellipsoidal, equal shape
     - "EVV" = ellipsoidal, equal volume (*)
     - "VVV" = ellipsoidal, varying volume, shape, and orientation

**** ~mclust~ output

     [[./L_Pics/Mixtures_pt2_3.png]]
     
     - Fits models with all types of constraints and chooses model with highest BIC (df penalty)
     - Many times, we fit a pre-specified type of model and vary the number of classes
     
*** Entropy as Evaluation
    A measure of classification quality that ranges between 0 and 1. A value of 1 indicates that individuals are easy to classify and a value of 0 indicates that individuals are randomly classified.

    $$entropy = 1 + \frac{1}{Nlog(k)}\big(\sum_{i=1}^{N}\sum_{k=1}^{K}P(C=k|x_i)log(P(C=k|x_i))   \big)$$

    Values of 0.9-0.8 are considered to be quite good.

*** Summary

    Mixture models are a flexible tool to model heterogeneity by deriving a probabilistic membership to class.

    Some drawbacks:
    - Can be tough to describe the classes
    - Tools in R are somewhat simpls
    - Can be tough to tell when overfitting, particularly in the case of skewed data

    Some of these concerns are ameliorated when pairing with other types of models, such as SEM, allowing for more theory and description.

** Factor Analysis
  
*** Outline
    The focus of this presentation will be:

    1. Introduce Exploratory Factor Analysis
    2. Differentiate it from Principal Components Analysis
    3. Specify Necessary EFA Details
    4. Go Through the Model Fitting Steps

*** FA Intro

    - We are going to take a somewhat circuitous route to understanding factor analysis. Instead of starting with the foundation of factor analysis, we are going to start with why we would use it.
    - Simply put, it answers the question of: How many constructs underly the scale I created? We will be using the Holzinger-Swineford dataset, a cognitive scale of 9 items (scales). Our question that we will answer is whether this scale measures a singular construct (i.e. intelligence is a singular construct), or whether it is multidimensional in nature. 

*** Holzinger Swineford

    For this presentation we will be using the "classic" Holzinger-Swineford (1939) dataset. It is useful because it has a pretty well established factor structure, but there is some uncertainty about the fit.

    You can access it from Mplus (24 scales), but we will be using the version from the lavaan package (9 scales).

    - x1 Visual perception
    - x2 Cubes
    - x3 Lozenges
    - x4 Paragraph comprehension
    - x5 Sentence completion
    - x6 Word meaning
    - x7 Speeded addition
    - x8 Speeded counting of dots
    - x9 Speeded discrimination straight and curved capitals

    Before we use statistical methods to determine how many factors underly these 9 scales, let's examine the correlation matrix.

    [[./L_Pics/FA_1.png]]

*** What is a Latent Factor?

    The basis for methods suchs as exploratory factor analysis, confirmatory factor analysis, and structural equation modeling is posing the existence of latent factors.

    In essence, what we are doing is saying that some "construct" underlies a set of observed variables. The simplest case is that of intelligence. Every (valid) IQ test is the result of some form of factor analysis. By measuring a host of scales, such as picture completion, vocabulary, arithmetic etc... we are saying that the correlation between these variables is not a result of chance occurrence, but the overlap between scales can be attributed to a "common factor".

    The use of factor analysis "tests" whether there are latent factors underlying a correlation matrix (set of variables).

*** Latent Factors in the Holzinger Swineford Dataset
    The Holzinger Swineford dataset is made up of cognitive scales. For simplicity sake, let's say that these can measure intelligence. The fundamental question we are answering with factor analysis with this dataset is whether there is a latent factor or latent factors underlying it. This can go back to the fundamental debate on whether IQ is a singular construct or multidimensional? We can answer this question with factor analysis.

    With 9 scales, we are limited in the number of latent factors we can "search" for. 

    [[./L_Pics/FA_2.png]]

*** Two Approaches to FA
    1. Confirmatory
       - If you have a hypothesis about the factor structure. 
    2. Explanatory
       - Otherwise, do exploratory FA

*** The Process
    The questions we need to answer are:
    1. How many latent factors underly HS?
    2. What can we call these latent factors?
    3. What scales load on which factors?
    4. How well does model fit?

    While also accounting for numerous other issues in applying EFA, we will attempt to answer all of these questions eventually.

*** Difference between CFA and EFA

    Factor Loading Structure (Simple Structure)
    CFA:

|       | Latent Factors |     |     |
|-------+----------------+-----+-----|
| Items | F1             | F2  | F3  |
|-------+----------------+-----+-----|
| X1    | 1.0            | -   | -   |
| X2    | 0.2            | -   | -   |
| X3    | 1.0            | -   | -   |
| X4    | -              | 1   | -   |
| X5    | -              | 0.2 | -   |
| X6    | -              | 1   | -   |
| X7    | -              | -   | 1   |
| X8    | -              | -   | 0.2 |
| X9    | -              | -   | 1   |

    EFA:
    #+begin_src R
    fa.out <- factanal(HS, 3)
    print(fa.out$loadings, cutoff = 0)
    #+end_src

    [[.L_Pics/FA_3.png]]

    In CFA, I am making hypotheses about 2 things:

    1. The number of factors underlying my data
    2. Which items (scales) load onto which factors

    Point #2 denotes a hypothesis we are making about each and every factor loadings.

    Additionally, we are making hypotheses about whether the factors are correlated. If we have three factors, we can test whether they are all uncorrelated, all correlated, or only two are correlated.

    The important point about CFA is that we are setting specific parameters in the model to zero, and testing whether the lack of these relationships negatively influences the fit of our model.

    #+begin_src R
    mod <- 'f1 =~ x1 + x2 + x3
            f2 =~ x4 + x5 + x6
            f1 ~~ 0*f2'

    out <- cfa(mod, HS)
    semPlot::semPaths(out, exoCov = F)
    #+end_src

*** EFA Fit

    In EFA, the only hypothesis we are making regarding our model is the number of factors. Given the nature of EFA estimation, whether the factors are uncorrelated (orthogonal) or correlated (oblique) does not change the fit of the model.

    Additionally, each manifest (observed) variable loads on to each latent factor.

    Technically speaking then, the only way to have misfit in EFA is for there to be residual covariance between variables that is not explained by the latent factors. 

    *Important Point* This means that we did not extract enough latent variables -- as a covariance between uniequenesses is essentially a latent variable.

*** EFA Explanation Part 1

    To go into detail about what EFA is doing, we are positing two things account for our manifest variables:
    1. the influence of latent factors
    2. unqiue variances

    The fundamental equation of factor analysis:

    $$Y = \Lambda X + \Psi E$$

    In english, this means that are manifest variables ($Y$) are weighted combinations ($\Lambda$ \& $\Psi$ are the weights) of the common factors in $X$ and the unique factors in $E$.

    However, most of the time we don't try and explain the actual data matrix, but instead the correlation matrix:

    $$R_{YY} = \Lambda R_{XX} \Lambda' + \Psi^{2}$$

    where $R_{XX}$ is the correlation (covariance) matrix among the latent variables. Typically the latent factor variances are constrained to 1. This allows us to estimate all of the factor loadings.
    
*** Latent Factor Influence

    So why the hell does that arrow go from the latent factors to the manifest variables?
    #+begin_src R
    mod <- 'f1 =~ x1 + x2 + x3 + x4 + x5'
    out <- cfa(mod,HS)
    semPlot::semPaths(out)
    #+end_src

    [[./L_Pics/FA_5.png]]
    
    In PCA, these arrows would be flipped around.

    [[./L_Pics/FA_6.png]]

* Week 12: Measurment and ML
* Week 13: Searching for Groups
** Clustering
   <2019-11-05 Tue 09:30-10:45> 
*** Overview
    
    - Unsupervised learning
    - Person-centered approach
    - Cluster Analysis
      - Ward's hierarchical clustering
      - K-means clustering
    - Empirical example

**** Supervised Learning

     - So far, we've discussed supervised learning methods (e.g., regression, classification)
     - We have access to a set of p variables $x_1$ to $x_p$, measured on n observations, and a response y also measured on those same observations.
     - Goal is to predict $y$ using $X$
      
     This is a well-understood area of statistics.
     - For instance, if you are asked to predict a binary outcome from a data set, you have a very well developed set of tools (e.g., logistic regression, classification trees) as well as a clear understanding of how to assess the quality of the results obtained (e.g., cross-validation, hold out sample)
      
**** Unsupervised Learning

     - A set of statistical tools intended for the setting in which we have a set of variables $X$ measured on n observations
     - We are not interested in prediction because we do not have an associated response variable $y$
     - The goal is to discover interesting things about the measurements on $X$
     - Is there an informative way to visualize the data?
       - Can we discover subgroups among the variables or among the observations?
     
     This is much more challenging than supervised learning
     - Tends to be more subjective
     - There is no simple goal for the analysis, such as prediction of a response
     - It can be hard to assess the results obtained from unsupervised learning methods because there is no universally accepted mechanism for validating the results
       - i.e. cannot check our work because we don't know the true answer. The problem is, appropos, unsupervised.
        
     However, these techniques for UL are of growing importance in a number of fields
     - A cancer researcher might assess gene expression levels in patients with breast cancer to look for subgroups among the breast cancer samples in order to obtain a better understanding of the disease
     - An online shopping site might try to identify groups of shoppers with simlar browsing and purchase histories to target coupons, sales, etc.
     - A search engine might choose what search results to display to a particular individual based on the click histories of other individuals with similar search patterns

**** The Person-Centered Approach

     - The modern person-centered approach is presented in Bergman and Magnusson (1997) and in Bergman, von Eye, and Magnusson (2006)
     - The approach has two facets: (1) a theoretical facet and (2) a methodological facet in the form of methods that most naturally are useful in implementing person-oriented research (usually pattern oriented methods)
     - In the holistic-interactionistic research paradigm the individual is seen as an organized whole with elements operating together to achieve a functioning system in a dynamic process with interactions between components
     - Components could be of different kinds (e.g., behaviors, biological factors, environmental factors)
     - This view is related to a dynamical systems view that exists in many other sciences
     - The paradigm can be applied to almost all studies of individuals, guiding problem formulation, research strategy and methodology, and guiding how to interpret findings

**** Tenents

     - Conceptual model of the developmental process is the one proposed by the holistic-interactionistic research paradigm. Hence, the process is partly individual-specific.
     - The developmental process follows laws that relate to structures organized and functioning as patterns of operating factors
       - These laws are assumed to have communalities across individuals but they cannot be assumed to be identical across individuals.
     - In the developmental process, typical patterns of observed system components often emerge both within the individual and across individuals.
       - Although there is an infinite variety of differences with regard to process characteristics and observed states at a detailed level, there will often be a small number of more frequently observed patterns

**** Methodological Considerations

     - The methodology should allow for inferences about the single person
     - The methodology should allow for inferences about individual patterns of functioning. This can normally only be accomplished by treating the key pattern defining the system of interest (usually a vector of variable values) as an indivisible unit in the analysis

*** Cluster Analysis
    
**** What is it?

    - Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters)
    - We seek to partition observations into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other.
    - To make this concrete, we must define what it means for two or more observations to be similar or different.

**** Clustering Algorithms

     Many clustering algorithms exist. The two most popular are:
     - Ward's hierarchical clustering
     - K-means clustering

**** Different approaches
     
    - K-means clustering
      - Seek to partition the observations into a pre-specified number of clusters
      - Top-down approach

    - Hierarchical clustering
      - We do not know in advance how many clusters we want
      - Bottom-up approach (grouping similar observations together)
      - End up with a tree-like visual representation of the observations called a dendrogram

*** Hierarchical Clustering
    
**** What is it?

    - Extremely simple algorithm
    - Begin by defining some sort of dissimilarity measure between each pair of observations (e.g., Euclidean distance)
    - Algorithm proceeds iteratively
      - Each of the n observations is treated as its own cluster
      - The two clusters that are most similar are then fused so that there now are n−1 clusters
      - Repeat Step 2 until all of the observations belong to one single cluster

    - The concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations
    - This extension is achieved by developing the notion of linkage, which defines the dissimilarity between two groups of observations.
      - Four most common types of linkage—complete, average, single, and centroid

**** Linkages

    - Complete
      - Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.
    - Single
      - Minimal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time.
    - Average
      - Mean intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.
    - Centroid
      - Dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.

**** Euclidean Distance

     - $d(x, y) = \sqrt{(x_2 - x_1)^2 + (y_x - y_1)^2}$
     - Sum of squares of difference on x plus sum of squares of differences on y
     - Biased by variables with a larger scale -> requires a standardization of data

**** Dissimilarity Measures

     - The choice of dissimilarity measure is very important, as it has a strong effect on the resulting clusters
     - An alternative dissimilarity measure is a correlation-based distance
       - Considers two observations to be similar if their features are highly correlated, even though the observed values may be far apart int erms of Euclidean distance
         
**** Correlation vs Euclidean

     [[./L_Pics/Clustering_1.png]]

**** Interpreting Dendrograms

    - Each leaf of the dendrogram represents one of the n observations
    - Moving up the tree, leaves begin to fuse into branches corresponding to observations that are similar to each other
    - Moving higher up, branches fuse, either with leaves or other branches
    - The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other
    - Observations that fuse later (near the top of the tree) can be quite different
      
    - The height of the fusion, as measured on the vertical axis, indicates how different the two observations are
      - Observations that fuse at the very bottom of the tree are quite similar to each other
      - Observations that fuse close to the top of the tree will tend to be quite different
    - Draw conclusions about the similarity of two observations based on the location on the vertical axis where branches containing those two observations first are fused

**** Picking a Number of Clusters from the Dendrogram

    - Make a horizontal cut across the dendrogram
    - The distinct sets of observations beneath the cut can be interpreted as clusters
    - Therefore a dendrogram can be used to obtain any number of clusters
    - Researchers often look at the dendrogram and select by eye a sensible number of clusters based on the heights of the fusion and the number of clusters desired
    - The choice of where to cut the dendrogram is not always clear.

     [[./L_Pics/Clustering_2.png]]

*** K-Means Clustering
    
**** What is it?
     
     - First, specify the desired number of clusters K
     - The K-means algorithm will assign each observation to exactly one of the K clusters

**** Clustering with K = 2, 3, and 4

     [[./L_Pics/Clustering_3.png]]

**** Notation

    - C1, ..., CK denote sets containing the indices of the observations in each cluster satisfying two properties
    - $C1 \cup C2 \cup ... \cup CK = 1, ..., n$
    - $Ck \cap ck \dne 0 for all k \dne k$
    - Each observation belongs to at least one of the K clusters
    - Clusters are nonoverlapping: no observation belongs to more than one cluster
      
**** What's the goal?

    - Good clustering is one for which the within-cluster variation is as small as possible
    - The within-cluster variation for cluster CK is a measure, W(CK) , of the amount by which the observations within a cluster differ from each other
    - We want to solve
    - $minimize$   $\sum_{k=1}^{K} W(C_k)$
    - Goal is to partition the observations into 𝐾 clusters, such that the total within-cluster variation, summed over all 𝐾 clusters, is as small as possible.

**** Definition of Within-Cluster Variation

     Most commonly used metric is the squared Euclidian distance
 
     $W(C_k) = \frac{1}{|C_k|}\sum_{i \in C_k}\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2$

     CK denotes the number of observations in cluster k

**** Algorithm

    - Randomly assign a number, from 1 to 𝐾, to each of the observations. These serve as initial cluster assignments for the observations.
    - Iterate until the cluster assignments stop changing:
      - For each of the 𝐾 clusters, compute the cluster centroid. The  𝑘 𝑡ℎ  cluster centroid is the vector of the p feature means for the observations in the  𝑘 𝑡ℎ  cluster
      - Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance)

**** Snapshot of Algorithm

     [[./L_Pics/Clustering_4.png]]

**** Local Optimum
     - The K-means algorithm finds a local rather than global optimum
     - Results obtained will depend on the initial (random) cluster assignment of each observation
     - It is important to run the algorithm multiple times from different random starting values and then select the best solution

     [[./L_Pics/Clustering_5.png]]
     
*** Conclusion
    
**** Issues in Clustering
     - Determining the number of clusters to retain
     - Cross-validation of clusters and cluster sizes
     - All or none decision process
       - Either in or out of a cluster
     - What to do with observations that really don’t belong in any cluster
     - Consequences of choices among linkage, dissimilarity measure, cutting dendrogram
     - Sensitivity to data pertubations

**** Recommendations
     - Performing clustering with different choices of parameters, and looking at the full set of results in order to see what patterns consistently emerge
     - Since clustering can be non-robust, we recommend clustering subsets of the data in order to get a sense of the robustness of the clusters obtained
     - Most importantly, we must be careful about how the results of a clustering analysis are reported.
       - Results should not be taken as the absolute truth about a data set
       - Constitute a starting point for the development of a scientific hypothesis and further study, preferably on an independent data set

** Principal Components Analysis
*** Principal Components Equations

    In contrast to EFA, the equation for PCA is:

    $$Y = \Lambda X$$
    
    ... where:

    $$\Lambda = A D^{1/2}$$
    
    ... with A as the eigenvector matrix, and D is a diagonal eigenvalue matrix.

*** Difference from FA

    [[./L_Pics/PCA_1.png]]
    
    In contrast to EFA, in PCA you can extract the same number of components as variables. However, in most data reduction settings, usually the minimum number of components to explain either 95% or 99% of the variance are extracted.


    For those in data mining, the term SVD (singular value decomposition), or really any other type of decomposition refers to an identical process. They are just transformations of eachother.

    In psychology, when dealing with a dataset that has observations (people), PCA is not nearly as useful as EFA. This is for couple reasons:

    1. Components aren't latent factors. We can't make statements about the construct they could represent
    2. Factors are akin to true scores, whereas components combined true and error scores
    3. We can't test hypotheses regarding the number of components. No model fit as in EFA
       
    *Take Home Message:* Compoenets = common variance + unique variance

*** Applying PCA

    #+begin_src R
    fit <- princomp(HS)
    summary(fit) # print variance accounted for
    
    plot(fit) # View the variance explained by componenets
    
    loadings(fit) # Pc loadings
    #+end_src

*** Parallel Analysis

    Even better is to use parallel analysis:

    #+begin_src R
    fa.parallele(HS) # from psych package
    #+end_src
    
    [[./L_Pics/PCA_2.png]]

* Week 14: Longitudinal Data
  <2019-11-19 Tue 09:30-10:45>

** How do we impose a framework on longitudinal data so that we can make sense of it?
   What we usually do is extract two different factors for *one variable over time.*
   
   Diagram:

   [[./L_Pics/Long_1.png]]
   
   The two constructs we extract are 1. a factor which describes where people start and 2. a factor which describes how people change over time.
   
** Growth Model

   The growth model (Wishart, 1938) has long been the main analytic technique for studying change over time.

   - Model decomposes repeated measures data into a (more or less) smooth trajectory of true scores plus error scores 

   - Smooth trajectory is often a predetermined mathematical function of time
     - Generally, participants are expected to follow the same type of mathematical function (e.g., linear, logistic), but participants are allowed to have different parameters of the function
     - Error scores are often assumed to be normally distributed with constant variance over people and time
       
   - Commonly specified models can be estimated in either the structural equation modeling (SEM) or mixed-effects modeling (MLM) frameworks
     
   - Over time, fewer differences between the two frameworks exist (see Ghisletta & Lindenberger, 2004; Grimm, Ram, & Estabrook, 2017)
     - Truly nonlinear models can only be directly estimated in the nonlinear mixed-effects modeling framework (with an approximation of the integral of the likelihood function)

** General Specification

   For $i$ in 1 to $N$ and $t$ in 1 to $T$:

   $$y_{ti} = f(\boldsymbol{b}_i, x_{ti}) + e_{ti}$$
   
   - $y_{ti}$ outcome for person $i$ measured at time $t$
   - $f(\boldsymbol{b}_i, x_{ti})$ is a linear or nonlinear function, where /b_i/ is a vector of random coefficients for person /i/, and $x_{ti}$ is the value of the time metric for person /i/ at time /t/
     - $\boldsymbol{b}_i \text{~} N(\boldsymbol{\beta}, \boldsymbol{\psi})$

* Week 15: Text Mining
* Week 16: Social Networks
