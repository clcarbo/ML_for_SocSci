#+TITLE: Machine Learning for Social and Behavioral Research
#+AUTHOR: Christopher Carbonaro
#+EMAIL: ccarbona@nd.edu
#+TODO: TODO IN_PROGRESS | WAITING DONE
#+CATEGORY: ML
#+Professor: Dr. Ross Jacobucci
#+CourseListing: PSY-40122; PSY-60122; MDSC-40122
#+OfficeRmNm: Corbett E440
#+OfficeHrs: Wed. 10:30-11:30
* Introduction and Housekeeping
  <2019-08-27 Tue 09:20-10:45>

** TODO Reading and reflections                                    :Readings:
   DEADLINE: <2019-09-19 Thu +1w>
   :PROPERTIES:
   :LAST_REPEAT: [2019-09-12 Thu 10:29]
   :END:

   - State "DONE"       from "TODO"       [2019-09-12 Thu 10:29]
   - State "DONE"       from "TODO"       [2019-09-04 Wed 23:44]
** Machine Learning Terminology
   Note that we could have used any of the following terms:
   - Statistical Learning
   - Data Mining
   - Artificial Intelligence
   - Exploratory Data Mining
   - Big Data
   - Data Science
     
     Machine learning is not an algorithm (you already know this); it's a larger concept which can be applied in multiple different ways.
   
*** Algorithm vs Model
    Model in statistics typically refers to a "general model" which conveys how the data were generated. Think of it as:
    - Algorithm = Method used: Linear regression, random forests, decision trees: $Y = f(X) + e$
    - Model = Resulting relationship between X and Y after fitting (algorithmic model): $Y = \hat{Y} + e$

*** Exploratory Data Analysis
    ^ A term attributed to John Tukey's book. What if you don't begin your analysis with a hypothesis? This is in contrast to how research was conducted earlier i.e. as "Confirmatory Research." ML is not explanatory. It tries to get at /exactly/ what the causal factors are, but this leads to very complicated explanations.

*** Regression vs Classification
    - Regression: The outcome of interest is continuous. Either integer or interval (metric)
    - Classification: The outcome of interest is categorical. Either binary or multicategorical.

      Complications:
      Likert type variables - Just refer to as ordinal, but more classification. Can produce predicted probabilities (logistic regression) or produce class predictions (clustering).

      N.B. The model can't think. If your measurments are poor measurments, you will get poorly modeled results.

*** X's and Y's
    Most models will have a form of:
    $$Y = f(X) + e$$
    
    Right hand side:
    - X's
    - Predictors
    - Covariates
    - Inputs

    Left hand side:
    - Y
    - Outcome
    - Response

*** Do we have a Y?
    One major distinction in ML is between supervised and unsupervised learning.
    
    - Supervised: These is an outcome
    - Unsupervised: There isn't an outcome of interest
    - Semi-Supervised: Neural Network type learning

    Generally, unsupervised means we are doing data reduction on the X's.

*** Univariate vs Multivariate
    - Univariate: a single outcome
    - Multivariate: multiple outcomes
    - Multivariable: multiple predictors

    Most things in this course are univariate and multivariable, e.g. 
    $$Y = b_0 + b_1 X_1 + b_2 X_2 + e$$

*** Inference
    Inference is typically reserved for models that make assumptions about a population (asymptotics). Mostly this occurs in the context of linear models.

    An analogous term is /explanation/: We want to derive a model to explain the relaitonship between X's and Y.


    This is in contrast to purely predictive modeling (forecasting), often denoted by the use of "black box" algorithms.

*** Parametric vs Non-Parametric
    - Parametric: Making distributional assumptions and using a finitenumber of parameters.
    - Non-Parametric: Everything else, typically encapsulating most ofmachine learning

    Why make assumptions?
    - Requires a smaller sample size
    - Often faster
    - More interpretable
*** Bayes vs Frequentist
    The distinction of Bayes vs Frequentist when it comes to p-values, regression, and such is very different than in ML.

    In ML, there is less of a debate. Bayesian is mostly used to estimate the coefficients of the model, often times faster than traditional (frequentist) approaches. In ML, priors are typically less important and have less of an impact on the results (posterior).
** What is Big Data?
   <2019-08-29 Thu 09:30-10:45>
*** "Is my data big enough?"
    99% of the time: Yes and No. It will be large enough to use the methods but small enough to incur many difficulties.
   
    Beware overfitting (where you get answers which are not generalizing to other samples).
*** Cattell's Data Box
    Time is becoming increasingly important as a variable.

    We used to have problems with keeping study participants over long spans of time. This is now easier with ease of communication between the conductor of the study and the recipient. But time can even be near-continuous as a variable now, with constant updates for variables like location.
*** Big N, Small P
    This is traditional social and behavioral research: N > P.

    Let T represent time.
    If T = 1, this is cross-sectional.
    If T > 1, the study is longitudinal.
*** Small N, Big P
    This falls in line with genetics and neuro research.
    - Lare numbers of SNP's
    - Large numbers of voekls from MRI.

    This work presents a number of problems for traditional statistical algorithms. You /cannot/ run a linear regression on a sample where N < P.
*** Small N, Small P
    Both N and P don't provide enough information. So we needd large T (time).
  
    N = 1, idiographic approach.
    N > 1, ecological momentary assessment or other.
   
    Traditional statistical models need N = big (longitudinal), but recently it has been possible to do studies on an individual level (idiographic) by using large T. This gives the researcher enough observations to begin drawing conclusions.
*** Sources of Data
    ML algorithms allow us to analyze data from new sources:
    - genetics
    - neuro
    - text
    - social media

    One perspective: These new forms of assessment with replace traditional forms.
    Better prespective: These new forms of assessment will /supplement/ traditional forms.

    The same is true for ML vs Statistics -- ML can supplement our traditional approaches.
* Week 2: Theory and ML
  <2019-09-03 Tue 09:30-10:45> 
** Exploratory and Confirmatory Research
*** Confirmatory Research
    Definition: A common view of what constitutes confirmatory research is a seriesof a priori hypotheses followed by research design (experimental inmost cases) to test the hypotheses, gathering data, analysis,concluded with inductive inference (Jaeger & Halliday, 1998)

    - Confirmatory – Minimize Type I errors.
      - Goal is to find causal mechanisms. Stringent alpha values.
    - Exploratory – Minimize Type II errors. 
      
    Confirmatory research is a hallmark of science. Stating hypotheses,running a study or experiment to test these hypotheses, and theneither finding enough evidence to support or fail to support thehypothesis, is an efficient and important cornerstone to this practice.

    Think of confirmatory as working from theory to data, whereas exploratory can be thought of as working from data towards theory.

*** Confirmatory Applied to Statistical Methods
    In defining confirmatory research, it is common to apply this to specific algorithms/models:
    - ANOVA
    - Mediation Models
    - Structural Equation Models
    - Propensity Score Matching
    The common theme is that these are often defined in terms of /causal/ modeling.
*** Modeling vs Practices
    Confirmatory models can be used in exploratory ways.

    Theoretical modeling - how much theory is incorporated into the model being tested?
    Theoretical practices - how much does theory inform every other part? e.g. data cleaning, whether hypotheses come before analysis, etc...
*** Confirmation and the Replication Crisis
    Replication crisis -> New emphasis on confirmatory research practices (e.g., Wagenmakers et al., 2012).

    Biggest Takeaways:
    - Pre-registration
    - Reporting all analyses
    - Not generating hypotheses from the data analysis
    - Posting data and analysis scripts
    - Pre-stating what confounders (or control variables) are used

    Lieberman & Cunningham (2009) found the average issue of JPSP had an average of 93 statistical tests per paper.

*** Exploratory Research
    Definition: One consistent distinction is in the types of data aligned with each type of modeling, with confirmatory data analysis (CDA) using mostly experimental data, while exploratory data analysis (EDA) typically uses observational data (Good, 1983).

    EDA is mostly attributed to Tukey - non-hypothesis informed graphical exploration of data to tell a story.

*** Generating Hypotheses
    EDA is more concerend with observable data, as opposed to those from experiements (Good, 1983) - obtained informally, thus the methods are often more informal.

    EDA is mostly concerned with the /encouragement/ of hypothesis formulation (Good, 1983).

*** EDA Can Improve Hypotheses
    Even if a hypothesis is formed, tested, and not rejected, there is almost always room for improvement.

    Examples:
    - Model the residuals
    - Follow up traditional statistics with ML

    EDA is sometimes likened to detective work (per Tukey) as a mechanism to uncover those things missed through more restrictive methods.

*** Motivation for Exploratory Research
    Jaeger & Halliday (1998) - "Explicit hypotheses tested with confirmatory research usually do not spring from an intellectual void but instead are gained through exploratory research."

    Some sects of Philosophy of science hold this form of notion (hypothetico deductivists) -> "Thinking really really hard"

*** Using ML to Inform Confirmatory Research
    Using twitter to assess moral content on twitter, followed by an experiment that manipulated perceived moral differences (Dehghani et al., 2016).

    - First step: Machine learning identifies the most important aspects - Theory generation.
    - Second step: Follow up with theory-testing in the form of an experiment or observational study.

*** Splitting the Data
    One way to use the same data for EDA and CDA is to split it prior to analysis
    - EDA on first half of sample - can torture as much as you want.
    - CDA on second half using hypotheses generated on first
      
    *N.B.* This cuts your data set in half, reducing your statistical power.

    According to Prof. Jacobuccui, this is a bad idea. You are less likely to find something in the first place.

*** Data Reduction
    Descriptive statistics is one form of data reduction: "We must suppress some of the truth to communicate the truth" (p 286; Good, 1983).

    Using of plotting techniques such as stem-and-leaf or histogram, or something like PCA:

*** Wonky Statistics
    There is nothing wrong with EDA - Problems occur when EDA is passed off as CDA.

    Example: A p-value works under the assumption that this is the single test being used.

    E.g., Torturing the data until it confesses - and it will confess.

** Culture Shift
   <2019-09-05 Thu 09:30-10:45>
** DONE Homework Assignment 1
   DEADLINE: <2019-09-12 Thu 09:30>
   [[./Lab_HW_1_Writeup.Rmd][Here]] is the .Rmd file.
* Week 3: Diagnostics and Resampling
  <2019-09-10 Tue 09:30-10:45>
** Intro
   Tuning parameters: The settings we use for individual algorithms. Regression does not have tuning parameters, but things like random forests do. *N.B. You likely will not know which parameters are best suited for your task. The only way to find out is to run models.* Think of topic modeling as an example.
   
   Polynomial regression -- the power
   Regularization -- The penalty
   Decision Trees -- Complexity parameter (depth)
   Random Forests -- Number of trees
   
   Any other examples?
** $R$
   What we want: $_TR$ (denoting the /true/ $R$) i.e. what $R$ is in the population.

   Problem is, we only have a sample. Given this, the next best thing we can try and get an estimate of is $_TRS$, or shrunken $R$. This always falls below $_TR$, hence \textit{shrunken}.

   There are a number of ways to try and get an estimate of $_TRS$. Notably:

   - Cross-validation
   - Bootstrapping
** Why we can only get an estimate of $_TR$
   OLS is about maximizing the fit between $Y$ and $\hat{Y}$. This means being short-sighted: We don't think about other samples when creating the weights.

   This is fine and dandy because our estimates for $b_j$ are unbiased. But our model fit is biased -- We overfit our current sample.

   Big sample and small numbers of predictors: This is not a problem.

   Small sample and large number of predicts: Problem.
** An Example:
   If you add more predictors to a model, you will /always/ get a better $R^2$. However, your /adjusted/ $R^2$ may not increase. In fact, after a point, it will go down, as you are penalized for using too many predictors.

** Model Assessment vs Selection

   Model Assessment: In only running one model, or after choosing a final model, determining the most unbiased assessment of model fit, either on new data or what it would be on new data.

   Model Selection: Estimating the performance of multiple algorithms and choosing a final model among these.

** Validation Set Approach
   
   The first, and most popular way to conduct cross-validation is termed the /validation set/ approach.
   
   Randomly split the sample in half: 50% Training & 50% Test samples.

   - Training sample: Explore to your heart's desire.
   - Test sample: Treat best model parameters from training as !!! *fixed* !!! and find new model fit.

** Creating Predictions

   $$MSE_{holdout}=\frac{1}{n}\sum_{i=1}^{n}(y_{i,holdout}-\hat{f}_{training}(x_{i,holdout}))^{2}$$
   
   Important: The model is not re-estimated. We use the parameters to /create/ predictions.

** Validation Set Example

   I split a dataset into 3 parts.

   - 100 Training cases
   - 100 Validation cases
   - 2036 Test cases.

   In practice, just split 50-50 (Train-Test), don't create such a large Test dataset. This is just for demonstration.

** Performance
   - Training: $R^2 = 0.25$, $R^2_{adj} = 0.088$
   - Test: $R^2 = 0.12$
   - "Population": $R^2 = 0.19$, $R^2_{adj} = 0.18$

   With this approach, inference on the training set over-estimates the model fit, while the test sample under-estimates the fit. This is a common occurrence when starting with a small sample.

** An example
   
   You should *always* see some indication of how $R^2$ changed for your Test set and your Training set.

   If you graph this, you should see an inverted 'U' shaped curve. 

** Another Approach: k-Fold Cross Validation

   Another approach to cross-validation is termed /k-Fold/ CV, since the data is split into k number of partitions. Common to use either 5 or 10 fold. 

   Here, you split, for example, into 4/5ths training and 1/5th test. Then test how well it fits. Then shift your frame. E.g. Parts 1:4 are training, 5 is test. Then Parts 1:3 + 5 are training, 4 is test. Then Parts 1:2 + 4:5 are training, 3 is test, etc.

** Bootstrapping

   Very similar to k-fold cross-validation, bootstrapping changes one thing: samples are replaced.

   Those sample that are not selected, about 33%, are used as the holdout to "predict" on, similar to K-fold.

   Repeating this 100 times, we get an average $R^2$ of 0.056, and SE of 0.045.

   Note -- There are multiple forms of bootstrap sampling:
   1. Create bootstrap sample, then use not selected as a test
   2. Just use bootstrap sample to estimate a model

   The second form is often used to estimate standard errors.
   
** Which to Use?
   
   The validation set approach is the most commonly understood, especially in domains where Machine Learning is not well understood. Best for when you have a large initial sample and are unsure as to what method you want to use. 

   However, the validation approach results in a loss of power. If less than 1000 observations, better to use k-fold or boostrapping. If < 100, bootstrapping is better.

   *Don't take these cutoffs as absolute truth, it depends on method used and number of variables.*

   Most don't have a large enough sample size. Leading to:
   - Lack of power = underfitting

** Model Selection

   Let's say I want to compare multiple algorithms: linear regression, decision trees, and random forests.

   - I use 5-fold for model assessment
   - Just selecting the model with the lowest 5-fold Mean Standard Error (MSE) incurs a small degree of bias
   - Larger bias with more algorithms and tuning parameters used.

   This is known as /optimization bias/.
   
   This is what happens when you conflate model assessment and selection.

** Nested Cross-Validation

   Use two loops.

   Begin by performing cross validation on your training set. Then conduct cross-validation on your whole sample i.e. check your test data on your entire training data.
** Using the ~Caret~ package
*** Demo Script from Professor Jacobucci

    #+begin_src R
    library(caret)
    library(AppliedPredictiveModeling)
    library(partykit)
    set.seed(12345)
    
    # Building a sample dataset
    lat.var1 = rnorm(300)
    lat.var2 = rnorm(300)

    ## Getting an output from the predictors
    y = 0.5*lat.var1 + 0.5*lat.var2 + rnorm(300,0,1)

    ## lm() trains a linear model
    summary(lm(y ~ lat.var1 + lat.var2))

    ## Putting the data into a df
    dat1 = data.frame(y=y,x1=lat.var1,x2=lat.var2)



    # nested CV demo
    outer.repeat = 5
    method.select = rep(NA,
                        outer.repeat)
    rsq = rep(NA,
              outer.repeat)
  
    # Creating the Partitions for CV
    ## This is for the outer loop
    folds.samp1 = createFolds(dat1$y,
                              k=outer.repeat,
                              list=F) 

    # CV
    #folds.samp1 = createResample(dat1$y,times=outer.repeat,list=F) # boot -- increase number of repeats
    # Looping through the outer partitions
for(j in 1:outer.repeat){
  lm.out = train(y ~ .,
                 dat1[folds.samp1!=j,],
                 method="lm",
                 trControl=trainControl(method="cv")) # default is 10-fold
    
    
  tree.out = train(y ~ .,
                   dat1[folds.samp1!=j,],
                   method="ctree",
                   trControl=trainControl(method="cv"),
                   tuneLength=1)
    
  ## Checking the R-Squared for partitions within each outer partition. N.B. Train() is partitioning and doing CV for you behind the scene
  if (lm.out$results$Rsquared > tree.out$results$Rsquared){
      method.select[j] = "lm"
      lm.out.test = train(y ~., dat1[folds.samp1!=j,],method="lm",trControl=trainControl(method="none")) # none uses whole data
      rsq[j] = cor(dat1[folds.samp1==j,"y"],predict(lm.out.test,dat1[folds.samp1==j,]))**2
    }else{
      method.select[j] = "ctree"
      ctree.out.test = train(y ~., dat1[folds.samp1!=j,],method="ctree",trControl=trainControl(method="none"))
      rsq[j] = cor(dat1[folds.samp1==j,"y"],predict(ctree.out.test,dat1[folds.samp1==j,]))**2
    }
    
}

rsq
method.select

    #+end_src
    
** Stratified CV

   Class imbalance can cause a multitude of problems (think, what happens if you are breaking up samples and only 5% of your cases are of one category? You might get a partition without any cases for that category). One solution is Stratified CV:
   - K-fold CV with pre-allocate positive cases to ensure class balance
     
   Inconsistent results to show whether it helps. Better option is up/down sampling, which we'll cover later in the semester.

** Repeated CV

   The pre-allocation of fold partitions can impart some bias. To reduce this, repeated CV has been proposed.
   
   Entails repeated 5 or 10 fold CV multiple times (e.g., 10) to ensure adequate randomness in the resulting partitions.

** Leave-One-Out CV

   An older form of cross-validation, and maybe the simplest is leave-one-out CV (LOOCV).
   - Train on N-1, Test on 1
   - Repeat N times

   Doesn't have good of a balance of bias and variance, but will see in some small sample studies -- MRI research.

** Fit Metrics
   There are a host of different fit criteria that try and approximate bias incurred by overfitting:

   Fit = within sample fit + complexity penalty


   The most well known:
   - Bayesian Information Criterion -- penalizes for # of parameters and N
   - Akaike Information Criterion -- penalizes for # of parameters 

   Not available for most ML algorithms and only useful in a relative sense. They must compute a log likelihood ($ln(\text{likelihood})$).

** TODO Homework Assignment 2
   DEADLINE: <2019-09-19 Thu 09:30>
* Week 4: Regression
** Classification Metrics
   <2019-09-17 Tue 09:30-10:45>
*** Classification Definition

    Classification = modeling a categorical outcome
    - binary
    - multicategorical

    Binary is far more common, and most metris will generalize to multicategorical outcomes.

    Most algorithms output a predict probbability of belonging to class 1. - Clustering and others = direct assignment to class

*** Goals
    Instead of focusing on objective functions for categorical outcomes, our goal is to answer the following question: How well did our model do in predicting a categorical outcome?

    In this, we run various algorithms and create predictions. Then comparing predictions to the actual outcomes.

*** Accuracy

    $$\text{Accuracy} = \frac{1}{n}\sum_{i=1}^{n}I(y_{i}=\hat{y}_{i})$$

    What percentage of observations do we accurately classify?
    
*** Example
    We are going to try and model whether someone has endorsed depressive symptoms.
    - Use a cutscore on the BDI from the epi.bfi dataset in the psych package.

    Just use the five Big Five Inventory (BFI) scores as predictors.
    - Agreeableness, Conscientiousnous, Extraversion, Neuroticism, Openness

*** Predicted Probabilities

    [[./L_Pics/Pred_Class_Model.png]]

*** Calibration Plot

    [[./L_Pics/Calib_Plot_1.png]]

*** Confusion Matrix

    [[./L_Pics/Conf_Mat_1.png]]

    Sensitivity can be thought of as the total number of predicted positive cases divided by the /actual/ number of positive cases.

    Specificity is the counterpart of sensitivity: it is the total number of predicted negatives divided by the /actual/ number of negatives.


    |                   | Actual Outcome |              |               |
    |                   | Positive       | Negative     |               |
    |-------------------+----------------+--------------+---------------+
    | Predicted Outcome | Positive       | TP           | FP ($\alpha$) |
    |                   | Negative       | FN ($\beta$) | TN            |

*** Balanced Accuracy
    <2019-09-19 Thu 09:30-10:45>
    
    This gives you the average across both classes.

    |            | Actual |     |
    | Prediction |    Yes |  No |
    |------------+--------+-----|
    | Yes        |     14 |   8 |
    | No         |     39 | 170 |
    
    - For Yes: $14/(14+39) = 0.26$
    - For No: $170/(170+8) = 0.955$
    - Average = 0.61

      [[./L_Pics/Sens_and_Spec.png]]

*** Kappa
    We can eyeball or use other statistics to compare Accuracy to the no information rate (1-prob(class 1)), but there are other statistics that measure our ability to predict beyond chance.

    $$\text{Kappa} = \frac{O - E}{1 - E}$$

    Where $O$ is the observed Accuracy and $E$ is the expected accuracy based on the marginal totals.

    $$E =  \frac{(Predicted_{No}*Actual_{No})}{N^2} + \frac{(Predicted_{Yes}*Actual_{Yes})}{N^2}$$

    Values > 0 mean an improvement above and beyond chance.

*** Creating Cutoffs

    If you don't specify, most programs will use a cutoff of 0.5. Which, in many cases, makes sense.

    How about in our example? Our baserate is 0.22.

** Probability Metrics

*** Area Under the Curve
    The area under the receiver operating characteristic curve (AUC) can be characterized as the probability that a randomly drawn positive case has a higher probability than a randomly drawn negative case (Fawcett, 2006).

    [[./L_Pics/AUC1.png]]
    
    For this picture, the gradient is your selected cutoff rate for classifying outcomes. Sensitivity is how well it identifies positive cases. FPR is your false positive rate i.e. how frequently you erroneously predict a positive when it is negative.

    An AUC of 1 means you have perfect specificity and sensitivity i.e. you always predict correctly. An AUC of 0.5 would mean you are doing the as well as random chance would e.g. a line from one corner to the other. Less than 0.5 means you are doing /worse/ than random chance.

    Here's an example of the area under a curve for two models: Logistic and Random Forest. RF fits perfectly because there was no cross-validation.

    [[./L_Pics/AUC2.png]]

*** Problems with AUC
    Class Imbalance is a major drawback (Saito \& Rehmsmeier, 2015)
    - High imbalance will inflate the AUC

   In the suicide example before, our AUC was 0.99 in most models.
   - Even though we only found tiny effects

   Also, need a test set to get actual curve.
   - Can calculate as part of 5-fold

*** PRAUC

    Let's invert our approach: how well do we predict the positives?

    [[./L_Pics/PRAUC.png]]
    
    Unlike AUC, there is no hard and fast value to look for e.g. 0.5 is equivalent to chance. PRAUC is all relative to other models.

** Introduction to Regularization

*** Variable Selection
    Regression uses the predictors to estimate $b_j$ to maximize $R^2$. But this is within sample.

    In some scenarios, removing predictors will improve our ability to accurately estimate $_TRS^2$.

    Earlier we used CV and bootstrapping to create a better estimate of $_TRS^2$, now we want to use variable selection with CV/bootstrapping to maximize $_TRS^2$.

*** Stepwise Methods: Best Subsets Selection

    To perform best subsets selection our goal is to find the /optimal/ configuration of variables, testing /all/ possible configurations.

    Number tested equals $2^k$, where $k$ is number of variables.

    Possible configurations:
		- 5 variables = 32
		- 20 = 1,048,576
		- 100 = 1.267651e+30

    Point being? It can take a while. So we need heuristics.

*** Backward Elimination

    Backwards, start with the full model (all predictors).

    In each step, remove the variable that lowers $R^2$ the least.

    The key: What metric to use to choose a final model?

*** Information Criteria
    
    Two really popular information criteria:

    Bayesian Information Criteria (BIC; Schwarz, 1978)

    $$BIC = N*log(RSS/N) + k*log(N)$$

    Akaike Information Criteria (AIC; Akaike, 1973)

    $$AIC = N*log(RSS/N) + 2k$$

    Which is equivalent to Mallow's Cp in regression.

    The lower the better. Both have similar idea to $R^2_{adj}$, but can also be used in non-nested models (but need same people).

    E.g.

    [[./L_Pics/Backwards.png]]

*** Selecting Final Model
    Pick the simplest model with the lowest BIC e.g. Intercept, C2, C3, E3, E4, N2, O2, gender.

*** Holdout
    In using stepwise selection, do not make inferences on the sample used to select variables! P-values are no longer valid. If you have a holdout, you can test the new subsetted model on the holdout and make inferences.

    Thus, I re-run the model on the test set and can examine p-values. 

    No test set = no p-values. Sorry

*** Forward Stepwise Regression
    Start with only an intercept, select the variable that has the largest correlation with Y (positive or negative).

    Next, add variable that increase $R^2$ the most. etc... 

    You can constrain it to have requirement that improvement is significant, but what do p-values mean anymore...

    Applied to our example, we get the same results.

*** A quote on stepwise methods:
    "Stepwise variable selection has been a very popular technique for many years, but if this procedure had just been proposed as a statistical method, it would most likely be rejected because it violates every principle of statistical estimation and hypothesis testing." (Harrel, 2015, p. 67).

*** Regularization
    Stepwise methods are not necessarily deficient, it is that the procedure invites too many opportunities for abuse.

    In contrast, we will discuss a number of methods under the umbrella of a method that is termed "regularization."

    This is a family of methods designed specifically for when the number of predictors grows beyond just a few.

*** Ridge Regression

    Ordinary least squares can be represented as minimizing the residual sum of squares (RSS):

    $$RSS = argmin\Big\{\sum_{i=1}^{N}(y_{i}-b_{0}-\sum_{j=1}^{p}x_{ij}b_{j})^{2}\Big\}$$

    From this, we build a penalty term:

    $$ridge=\underbrace{RSS}_{OLS}+\underbrace{\lambda\sum_{j=1}^{p}b_{j}^{2}}_{ridge}$$

*** Lasso Regression
    Specifically to /select/ variables, the least absolute shinkage and selection operator (lasso; Tibshirani, 1996, 2011) is:

    $$lasso=\underbrace{RSS}_{OLS}+\underbrace{\lambda\sum_{j=1}^{p}|b_{j}|}_{lasso}$$

*** Alternative Equation

    $$lasso=\underbrace{RSS}_{OLS} \quad \text{subject to} \quad \sum |b_j| \le t$$

    $$ridge=\underbrace{RSS}_{OLS} \quad \text{subject to} \quad \sum b_j^2 \le t$$

    where $t$ is a tuning parameter similar to $\lambda$

*** Example 1

    [[./L_Pics/Reg_1.png]]

*** Example 2

    [[./L_Pics/Reg_2.png]]

*** Parameter Trajectory

    [[./L_Pics/Reg_3.png]]

*** Collinearity
    If I simulate two variables, $x_1$ and $x_2$, to both have $b_j$ of 1, but a correlation between them of 0.999, this is what occurs in linear regression.

    [[./L_Pics/Reg_4.png]]

*** Ridge for Collinearity
    
    |   | $\lambda$ |   x1 |   x2 |
    |---+-----------+------+------|
    | 1 |      0.00 | 1.44 | 0.15 |
    | 2 |       0.1 | 1.15 | 0.44 |
    | 3 |       0.2 | 1.04 | 0.55 |
    | 4 |       0.3 | 0.98 | 0.60 |
    | 5 |       1.0 | 0.86 | 0.72 |
    
    By penalizing both coefficients, it /shrinks/ the inflated coefficients and inflates the suppressed varaibles.

    In general, no easy remedy for collinearity.

*** Main Points

    Ridge regression:
    - Handles collinearity.

    Lasso regresson: 
    - Performs variable selection.

    As a way to combine these, Zou and Hastie (2005) proposed the /elastic net/ (enet) regularization. Through the use of a mixing parameter, $\alpha$, the elastic net combines both ridge and lasso regularization

    $$enet= \underbrace{RSS}_{OLS} + \underbrace{(1-\alpha)\lambda\sum_{j=1}^{p}\beta_{j}^{2}}_{ridge} + \underbrace{\alpha\lambda\sum_{j=1}^{p}|\beta_{j}|}_{lasso}$$

*** Elastic Net Note

    One drawback: generally not as sparse as the lasso
    
    Also, more computationally intensive.
    - Could just test value of 0.5
    - Or 0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1

    Typically worth doing manually with ~train()~.
    - glmnet won't test vector
      
*** Example Script from Prof. Jacobucci

    #+begin_src R
#ecls.1 = read.table('C:/Users/rjacobuc/Documents/GitHub/edm_book/ch5_trees/ecls_DM.dat', na='.');
#ecls.1 = read.table('/Users/rjacobuc/Documents/GitHub/edm_book/ch5_trees/scripts/ecls_DM.dat', na='.');
#ecls.1 = read.table('C:/Users/rjacobuc/Documents/GitHub/edm_book/ch5_trees/scripts/ecls_DM.dat', na='.');

#library(synthpop)

#ecls_syn <- syn(ecls.1)$syn
#write.csv(ecls_syn,"G:/My Drive/PSY-ML-Fall19/4.Scripts/ecls_syn.csv")

ecls_syn <- read.csv(file.choose())

names(ecls_syn) = c('gender','kage',
                  'k_read_irt','k_read1','k_read2','k_read3','k_read4',	
                  'k_print','k_read_tht',
                  'k_math_irt','k_math1','k_math2','k_math3','k_math4',
                  'k_math_tht',
                  'k_gk_irt','k_gk_tht',
                  'f_mtr','g_mtr',
                  'P1LEARN','P1CONTRO','P1SOCIAL','P1SADLON','P1IMPULS',	
                  'ars_lit','ars_mth','ars_gk',
                  'T1LEARN','T1CONTRO','T1INTERP','T1EXTERN','T1INTERN',	
                  'height','weight','bmi',
                  'hisp','na_amer','asian','black','pac_isl','white','m_race',
                  'ses_c','ses_cat','poor','income',
                  'g8_read','g8_read_tht','g8_math','g8_math_tht',
                  'g8_sci','g8_sci_tht')

# subset of variables
x.vars = c('k_math_irt','k_gk_irt',"income","bmi","gender")
set.seed(1)
x.noise <- matrix( rnorm(nrow(ecls_syn)*20,mean=0,sd=1), nrow(ecls_syn), 20) 
y.vars = 'g8_sci'
ecls_syn$gender = ecls_syn$gender - 1
XX <- data.matrix(cbind(ecls_syn[,x.vars],x.noise))
XX.std <- data.matrix(cbind(scale(ecls_syn[,x.vars]),x.noise))
YY <- as.numeric(scale(ecls_syn[,y.vars]))



# linear regression
dat.comb <- data.frame(YY,XX.std)
lm.out <- 






library(glmnet)
# ?glmnet
# return in original scale
lasso.out <- glmnet(XX,YY,family="gaussian",alpha=1)
plot(lasso.out)

lasso.cv <- cv.glmnet(XX,YY,family="gaussian",alpha=1)
plot(lasso.cv,size=2)

round(coef(lasso.cv,lasso.cv$lambda.1se),3)
round(coef(lasso.cv,lasso.cv$lambda.min),3)


relax.out <- lm(scale(g8_sci) ~ scale(income) + scale(k_math_irt) + scale(k_gk_irt),ecls_syn)
summary(relax.out)
round(coef(relax.out),3)

# ridge

ridge.out <- glmnet(XX,YY,family="gaussian",alpha=0)
plot(ridge.out)

ridge.cv <- cv.glmnet(XX,YY,family="gaussian",alpha=0)
plot(ridge.cv)
round(coef(ridge.cv,ridge.cv$lambda.1se),3)


#lasso.std <- glmnet(XX.std,YY,family="gaussian",alpha=1,intercept=F,standardize=F)
#plot(lasso.std)

# penalized package 
#library(penalized)
#fit1 <- penalized(YY, penalized=XX, unpenalized=~0, standardize=TRUE) 
#round(coefficients(fit1),3)


# lasso p-values

library(covTest) # have to install from cran arxiv https://cran.r-project.org/src/contrib/Archive/covTest/
a=lars.en(XX,YY,lambda2=0)
cov.out = covTest(a,XX,YY)
cov.out$results[1:4,1] # get predictor numbers

head(XX[,c(2,1,3,5)])




# hierarchical lasso
set.seed(1)
ids <- sample(1:nrow(XX.std),nrow(XX.std)*.5)
XX.train <- XX.std[ids,]
XX.test <- XX.std[-ids,]
YY.train <- YY[ids]
YY.test <- YY[-ids]

library(hierNet)

out = hierNet.path(XX.train,YY.train,strong=TRUE)
out.cv = hierNet.cv(out,XX.train,YY.train)

plot(out.cv)
out.cv

out$th[,,11] # interactions and quadratic effects
out$bp[,11] # positive main effects
out$bn[,11] # negative main effects

# test if weak identifies others.
out.weak = hierNet.path(XX.train,YY.train,strong=FALSE)
out.cv.weak = hierNet.cv(out.weak,XX.train,YY.train)
plot(out.cv.weak)
out.cv.weak

out.weak$th[,,11] # interactions
out.weak$bp[,11]
out.weak$bn[,11]


# re-run model

dat.comb <- data.frame(XX.test,YY.test)
colnames(dat.comb)[1:5] <- c("math","knowledge","income","bmi","gender")
colnames(dat.comb)[26] <- "science"

lm.int <- lm(science ~ math + knowledge + income + bmi + gender + 
               I(math^2) + I(knowledge^2) + I(income^2) + I(bmi^2) +
               math*knowledge + knowledge*income,
             dat.comb)
summary(lm.int)
stargazer::stargazer(lm.int,column.sep.width = "1pt",single.row = TRUE)




# try stability selection

# stability
library(stabs)
stab.out11 <- stabsel(XX,YY,cutoff=0.75,PFER=1)
stab.out11
    #+end_src

** Regularization and Bias-Variance
*** APE Exposure Data
    [[./L_Pics/APE_Data.png]]
*** 20th Order Polynomial
    [[./L_Pics/Reg_5.png]]
*** Random Forests
    [[./L_Pics/Reg_6.png]]
*** Bias vs. Variance
    [[./L_Pics/Bias_v_Var.png]]
*** Fit Decomposition
    $$MSE = Bias^{2} + Variance$$

    $$\mathop{\mathbb{E}} \left[ (y - \hat{f}(x))^2  \right] = \left(Bias[\hat{f}(x)] \right)^2 + Var[\hat{f}(x)] + \sigma^2$$

    $$Bias[\hat{f}(x)] = \mathop{\mathbb{E}}[\hat{f}(x)] - \mathop{\mathbb{E}}[f(x)]$$

    $$Variance[\hat{f}(x)] = \mathop{\mathbb{E}}[\hat{f}(x)^2] - \mathop{\mathbb{E}}[f(x)]^2$$

    Its about decomposing the expected error on an unseen sample (population).

    - Overly simple model = High Bias
    - Overly complex model = High variance
*** Decomposing

    [[./L_Pics/Decomp.png]]
*** Model Fit
    [[./L_Pics/Model_Fit.png]]

* Week 5: Regression (cont.)
* Week 6: Trees
* Week 7: Ensembles
* Week 8: Ensembles (cont.) and Project Proposals
* Week 9: No Class (Fall Break)
* Week 10: Ensembles (cont.)
* Week 11: Nonlinear methods
* Week 12: Measurment and ML
* Week 13: Searching for Groups
* Week 14: Longitudinal Data
* Week 15: Text Mining
* Week 16: Social Networks
