#+TITLE: Machine Learning for Social and Behavioral Research
#+AUTHOR: Christopher Carbonaro
#+EMAIL: ccarbona@nd.edu
#+TODO: TODO IN_PROGRESS | WAITING DONE
#+CATEGORY: ML
#+Professor: Dr. Ross Jacobucci
#+CourseListing: PSY-40122; PSY-60122; MDSC-40122
#+OfficeRmNm: Corbett E440
#+OfficeHrs: Wed. 10:30-11:30
* Introduction and Housekeeping
  <2019-08-27 Tue 09:20-10:45>

** TODO Reading and reflections                                    :Readings:
   DEADLINE: <2019-10-24 Thu +1w>
   :PROPERTIES:
   :LAST_REPEAT: [2019-10-10 Thu 10:29]
   :END:

   - State "DONE"       from "TODO"       [2019-10-10 Thu 10:29]
   - State "DONE"       from "TODO"       [2019-09-12 Thu 10:29]
   - State "DONE"       from "TODO"       [2019-09-04 Wed 23:44]
** Machine Learning Terminology
   Note that we could have used any of the following terms:
   - Statistical Learning
   - Data Mining
   - Artificial Intelligence
   - Exploratory Data Mining
   - Big Data
   - Data Science
     
     Machine learning is not an algorithm (you already know this); it's a larger concept which can be applied in multiple different ways.
   
*** Algorithm vs Model
    Model in statistics typically refers to a "general model" which conveys how the data were generated. Think of it as:
    - Algorithm = Method used: Linear regression, random forests, decision trees: $Y = f(X) + e$
    - Model = Resulting relationship between X and Y after fitting (algorithmic model): $Y = \hat{Y} + e$

*** Exploratory Data Analysis
    ^ A term attributed to John Tukey's book. What if you don't begin your analysis with a hypothesis? This is in contrast to how research was conducted earlier i.e. as "Confirmatory Research." ML is not explanatory. It tries to get at /exactly/ what the causal factors are, but this leads to very complicated explanations.

*** Regression vs Classification
    - Regression: The outcome of interest is continuous. Either integer or interval (metric)
    - Classification: The outcome of interest is categorical. Either binary or multicategorical.

      Complications:
      Likert type variables - Just refer to as ordinal, but more classification. Can produce predicted probabilities (logistic regression) or produce class predictions (clustering).

      N.B. The model can't think. If your measurments are poor measurments, you will get poorly modeled results.

*** X's and Y's
    Most models will have a form of:
    $$Y = f(X) + e$$
    
    Right hand side:
    - X's
    - Predictors
    - Covariates
    - Inputs

    Left hand side:
    - Y
    - Outcome
    - Response

*** Do we have a Y?
    One major distinction in ML is between supervised and unsupervised learning.
    
    - Supervised: These is an outcome
    - Unsupervised: There isn't an outcome of interest
    - Semi-Supervised: Neural Network type learning

    Generally, unsupervised means we are doing data reduction on the X's.

*** Univariate vs Multivariate
    - Univariate: a single outcome
    - Multivariate: multiple outcomes
    - Multivariable: multiple predictors

    Most things in this course are univariate and multivariable, e.g. 
    $$Y = b_0 + b_1 X_1 + b_2 X_2 + e$$

*** Inference
    Inference is typically reserved for models that make assumptions about a population (asymptotics). Mostly this occurs in the context of linear models.

    An analogous term is /explanation/: We want to derive a model to explain the relaitonship between X's and Y.


    This is in contrast to purely predictive modeling (forecasting), often denoted by the use of "black box" algorithms.

*** Parametric vs Non-Parametric
    - Parametric: Making distributional assumptions and using a finitenumber of parameters.
    - Non-Parametric: Everything else, typically encapsulating most ofmachine learning

    Why make assumptions?
    - Requires a smaller sample size
    - Often faster
    - More interpretable
*** Bayes vs Frequentist
    The distinction of Bayes vs Frequentist when it comes to p-values, regression, and such is very different than in ML.

    In ML, there is less of a debate. Bayesian is mostly used to estimate the coefficients of the model, often times faster than traditional (frequentist) approaches. In ML, priors are typically less important and have less of an impact on the results (posterior).
** What is Big Data?
   <2019-08-29 Thu 09:30-10:45>
*** "Is my data big enough?"
    99% of the time: Yes and No. It will be large enough to use the methods but small enough to incur many difficulties.
   
    Beware overfitting (where you get answers which are not generalizing to other samples).
*** Cattell's Data Box
    Time is becoming increasingly important as a variable.

    We used to have problems with keeping study participants over long spans of time. This is now easier with ease of communication between the conductor of the study and the recipient. But time can even be near-continuous as a variable now, with constant updates for variables like location.
*** Big N, Small P
    This is traditional social and behavioral research: N > P.

    Let T represent time.
    If T = 1, this is cross-sectional.
    If T > 1, the study is longitudinal.
*** Small N, Big P
    This falls in line with genetics and neuro research.
    - Lare numbers of SNP's
    - Large numbers of voekls from MRI.

    This work presents a number of problems for traditional statistical algorithms. You /cannot/ run a linear regression on a sample where N < P.
*** Small N, Small P
    Both N and P don't provide enough information. So we needd large T (time).
  
    N = 1, idiographic approach.
    N > 1, ecological momentary assessment or other.
   
    Traditional statistical models need N = big (longitudinal), but recently it has been possible to do studies on an individual level (idiographic) by using large T. This gives the researcher enough observations to begin drawing conclusions.
*** Sources of Data
    ML algorithms allow us to analyze data from new sources:
    - genetics
    - neuro
    - text
    - social media

    One perspective: These new forms of assessment with replace traditional forms.
    Better prespective: These new forms of assessment will /supplement/ traditional forms.

    The same is true for ML vs Statistics -- ML can supplement our traditional approaches.
** TODO Final Project Proposal
   DEADLINE: <2019-11-05 Tue 23:59>
* Week 2: Theory and ML
  <2019-09-03 Tue 09:30-10:45> 
** Exploratory and Confirmatory Research
*** Confirmatory Research
    Definition: A common view of what constitutes confirmatory research is a seriesof a priori hypotheses followed by research design (experimental inmost cases) to test the hypotheses, gathering data, analysis,concluded with inductive inference (Jaeger & Halliday, 1998)

    - Confirmatory – Minimize Type I errors.
      - Goal is to find causal mechanisms. Stringent alpha values.
    - Exploratory – Minimize Type II errors. 
      
    Confirmatory research is a hallmark of science. Stating hypotheses,running a study or experiment to test these hypotheses, and theneither finding enough evidence to support or fail to support thehypothesis, is an efficient and important cornerstone to this practice.

    Think of confirmatory as working from theory to data, whereas exploratory can be thought of as working from data towards theory.

*** Confirmatory Applied to Statistical Methods
    In defining confirmatory research, it is common to apply this to specific algorithms/models:
    - ANOVA
    - Mediation Models
    - Structural Equation Models
    - Propensity Score Matching
    The common theme is that these are often defined in terms of /causal/ modeling.
*** Modeling vs Practices
    Confirmatory models can be used in exploratory ways.

    Theoretical modeling - how much theory is incorporated into the model being tested?
    Theoretical practices - how much does theory inform every other part? e.g. data cleaning, whether hypotheses come before analysis, etc...
*** Confirmation and the Replication Crisis
    Replication crisis -> New emphasis on confirmatory research practices (e.g., Wagenmakers et al., 2012).

    Biggest Takeaways:
    - Pre-registration
    - Reporting all analyses
    - Not generating hypotheses from the data analysis
    - Posting data and analysis scripts
    - Pre-stating what confounders (or control variables) are used

    Lieberman & Cunningham (2009) found the average issue of JPSP had an average of 93 statistical tests per paper.

*** Exploratory Research
    Definition: One consistent distinction is in the types of data aligned with each type of modeling, with confirmatory data analysis (CDA) using mostly experimental data, while exploratory data analysis (EDA) typically uses observational data (Good, 1983).

    EDA is mostly attributed to Tukey - non-hypothesis informed graphical exploration of data to tell a story.

*** Generating Hypotheses
    EDA is more concerend with observable data, as opposed to those from experiements (Good, 1983) - obtained informally, thus the methods are often more informal.

    EDA is mostly concerned with the /encouragement/ of hypothesis formulation (Good, 1983).

*** EDA Can Improve Hypotheses
    Even if a hypothesis is formed, tested, and not rejected, there is almost always room for improvement.

    Examples:
    - Model the residuals
    - Follow up traditional statistics with ML

    EDA is sometimes likened to detective work (per Tukey) as a mechanism to uncover those things missed through more restrictive methods.

*** Motivation for Exploratory Research
    Jaeger & Halliday (1998) - "Explicit hypotheses tested with confirmatory research usually do not spring from an intellectual void but instead are gained through exploratory research."

    Some sects of Philosophy of science hold this form of notion (hypothetico deductivists) -> "Thinking really really hard"

*** Using ML to Inform Confirmatory Research
    Using twitter to assess moral content on twitter, followed by an experiment that manipulated perceived moral differences (Dehghani et al., 2016).

    - First step: Machine learning identifies the most important aspects - Theory generation.
    - Second step: Follow up with theory-testing in the form of an experiment or observational study.

*** Splitting the Data
    One way to use the same data for EDA and CDA is to split it prior to analysis
    - EDA on first half of sample - can torture as much as you want.
    - CDA on second half using hypotheses generated on first
      
    *N.B.* This cuts your data set in half, reducing your statistical power.

    According to Prof. Jacobuccui, this is a bad idea. You are less likely to find something in the first place.

*** Data Reduction
    Descriptive statistics is one form of data reduction: "We must suppress some of the truth to communicate the truth" (p 286; Good, 1983).

    Using of plotting techniques such as stem-and-leaf or histogram, or something like PCA:

*** Wonky Statistics
    There is nothing wrong with EDA - Problems occur when EDA is passed off as CDA.

    Example: A p-value works under the assumption that this is the single test being used.

    E.g., Torturing the data until it confesses - and it will confess.

** Culture Shift
   <2019-09-05 Thu 09:30-10:45>
** DONE Homework Assignment 1
   DEADLINE: <2019-09-12 Thu 09:30>
   [[./Lab_HW_1_Writeup.Rmd][Here]] is the .Rmd file.
* Week 3: Diagnostics and Resampling
  <2019-09-10 Tue 09:30-10:45>
** Intro
   Tuning parameters: The settings we use for individual algorithms. Regression does not have tuning parameters, but things like random forests do. *N.B. You likely will not know which parameters are best suited for your task. The only way to find out is to run models.* Think of topic modeling as an example.
   
   Polynomial regression -- the power
   Regularization -- The penalty
   Decision Trees -- Complexity parameter (depth)
   Random Forests -- Number of trees
   
   Any other examples?
** $R$
   What we want: $_TR$ (denoting the /true/ $R$) i.e. what $R$ is in the population.

   Problem is, we only have a sample. Given this, the next best thing we can try and get an estimate of is $_TRS$, or shrunken $R$. This always falls below $_TR$, hence \textit{shrunken}.

   There are a number of ways to try and get an estimate of $_TRS$. Notably:

   - Cross-validation
   - Bootstrapping
** Why we can only get an estimate of $_TR$
   OLS is about maximizing the fit between $Y$ and $\hat{Y}$. This means being short-sighted: We don't think about other samples when creating the weights.

   This is fine and dandy because our estimates for $b_j$ are unbiased. But our model fit is biased -- We overfit our current sample.

   Big sample and small numbers of predictors: This is not a problem.

   Small sample and large number of predicts: Problem.
** An Example:
   If you add more predictors to a model, you will /always/ get a better $R^2$. However, your /adjusted/ $R^2$ may not increase. In fact, after a point, it will go down, as you are penalized for using too many predictors.

** Model Assessment vs Selection

   Model Assessment: In only running one model, or after choosing a final model, determining the most unbiased assessment of model fit, either on new data or what it would be on new data.

   Model Selection: Estimating the performance of multiple algorithms and choosing a final model among these.

** Validation Set Approach
   
   The first, and most popular way to conduct cross-validation is termed the /validation set/ approach.
   
   Randomly split the sample in half: 50% Training & 50% Test samples.

   - Training sample: Explore to your heart's desire.
   - Test sample: Treat best model parameters from training as !!! *fixed* !!! and find new model fit.

** Creating Predictions

   $$MSE_{holdout}=\frac{1}{n}\sum_{i=1}^{n}(y_{i,holdout}-\hat{f}_{training}(x_{i,holdout}))^{2}$$
   
   Important: The model is not re-estimated. We use the parameters to /create/ predictions.

** Validation Set Example

   I split a dataset into 3 parts.

   - 100 Training cases
   - 100 Validation cases
   - 2036 Test cases.

   In practice, just split 50-50 (Train-Test), don't create such a large Test dataset. This is just for demonstration.

** Performance
   - Training: $R^2 = 0.25$, $R^2_{adj} = 0.088$
   - Test: $R^2 = 0.12$
   - "Population": $R^2 = 0.19$, $R^2_{adj} = 0.18$

   With this approach, inference on the training set over-estimates the model fit, while the test sample under-estimates the fit. This is a common occurrence when starting with a small sample.

** An example
   
   You should *always* see some indication of how $R^2$ changed for your Test set and your Training set.

   If you graph this, you should see an inverted 'U' shaped curve. 

** Another Approach: k-Fold Cross Validation

   Another approach to cross-validation is termed /k-Fold/ CV, since the data is split into k number of partitions. Common to use either 5 or 10 fold. 

   Here, you split, for example, into 4/5ths training and 1/5th test. Then test how well it fits. Then shift your frame. E.g. Parts 1:4 are training, 5 is test. Then Parts 1:3 + 5 are training, 4 is test. Then Parts 1:2 + 4:5 are training, 3 is test, etc.

** Bootstrapping

   Very similar to k-fold cross-validation, bootstrapping changes one thing: samples are replaced.

   Those sample that are not selected, about 33%, are used as the holdout to "predict" on, similar to K-fold.

   Repeating this 100 times, we get an average $R^2$ of 0.056, and SE of 0.045.

   Note -- There are multiple forms of bootstrap sampling:
   1. Create bootstrap sample, then use not selected as a test
   2. Just use bootstrap sample to estimate a model

   The second form is often used to estimate standard errors.
   
** Which to Use?
   
   The validation set approach is the most commonly understood, especially in domains where Machine Learning is not well understood. Best for when you have a large initial sample and are unsure as to what method you want to use. 

   However, the validation approach results in a loss of power. If less than 1000 observations, better to use k-fold or boostrapping. If < 100, bootstrapping is better.

   *Don't take these cutoffs as absolute truth, it depends on method used and number of variables.*

   Most don't have a large enough sample size. Leading to:
   - Lack of power = underfitting

** Model Selection

   Let's say I want to compare multiple algorithms: linear regression, decision trees, and random forests.

   - I use 5-fold for model assessment
   - Just selecting the model with the lowest 5-fold Mean Standard Error (MSE) incurs a small degree of bias
   - Larger bias with more algorithms and tuning parameters used.

   This is known as /optimization bias/.
   
   This is what happens when you conflate model assessment and selection.

** Nested Cross-Validation

   Use two loops.

   Begin by performing cross validation on your training set. Then conduct cross-validation on your whole sample i.e. check your test data on your entire training data.
** Using the ~Caret~ package
*** Demo Script from Professor Jacobucci

    #+begin_src R
    library(caret)
    library(AppliedPredictiveModeling)
    library(partykit)
    set.seed(12345)
    
    # Building a sample dataset
    lat.var1 = rnorm(300)
    lat.var2 = rnorm(300)

    ## Getting an output from the predictors
    y = 0.5*lat.var1 + 0.5*lat.var2 + rnorm(300,0,1)

    ## lm() trains a linear model
    summary(lm(y ~ lat.var1 + lat.var2))

    ## Putting the data into a df
    dat1 = data.frame(y=y,x1=lat.var1,x2=lat.var2)



    # nested CV demo
    outer.repeat = 5
    method.select = rep(NA,
                        outer.repeat)
    rsq = rep(NA,
              outer.repeat)
  
    # Creating the Partitions for CV
    ## This is for the outer loop
    folds.samp1 = createFolds(dat1$y,
                              k=outer.repeat,
                              list=F) 

    # CV
    #folds.samp1 = createResample(dat1$y,times=outer.repeat,list=F) # boot -- increase number of repeats
    # Looping through the outer partitions
for(j in 1:outer.repeat){
  lm.out = train(y ~ .,
                 dat1[folds.samp1!=j,],
                 method="lm",
                 trControl=trainControl(method="cv")) # default is 10-fold
    
    
  tree.out = train(y ~ .,
                   dat1[folds.samp1!=j,],
                   method="ctree",
                   trControl=trainControl(method="cv"),
                   tuneLength=1)
    
  ## Checking the R-Squared for partitions within each outer partition. N.B. Train() is partitioning and doing CV for you behind the scene
  if (lm.out$results$Rsquared > tree.out$results$Rsquared){
      method.select[j] = "lm"
      lm.out.test = train(y ~., dat1[folds.samp1!=j,],method="lm",trControl=trainControl(method="none")) # none uses whole data
      rsq[j] = cor(dat1[folds.samp1==j,"y"],predict(lm.out.test,dat1[folds.samp1==j,]))**2
    }else{
      method.select[j] = "ctree"
      ctree.out.test = train(y ~., dat1[folds.samp1!=j,],method="ctree",trControl=trainControl(method="none"))
      rsq[j] = cor(dat1[folds.samp1==j,"y"],predict(ctree.out.test,dat1[folds.samp1==j,]))**2
    }
    
}

rsq
method.select

    #+end_src
    
** Stratified CV

   Class imbalance can cause a multitude of problems (think, what happens if you are breaking up samples and only 5% of your cases are of one category? You might get a partition without any cases for that category). One solution is Stratified CV:
   - K-fold CV with pre-allocate positive cases to ensure class balance
     
   Inconsistent results to show whether it helps. Better option is up/down sampling, which we'll cover later in the semester.

** Repeated CV

   The pre-allocation of fold partitions can impart some bias. To reduce this, repeated CV has been proposed.
   
   Entails repeated 5 or 10 fold CV multiple times (e.g., 10) to ensure adequate randomness in the resulting partitions.

** Leave-One-Out CV

   An older form of cross-validation, and maybe the simplest is leave-one-out CV (LOOCV).
   - Train on N-1, Test on 1
   - Repeat N times

   Doesn't have good of a balance of bias and variance, but will see in some small sample studies -- MRI research.

** Fit Metrics
   There are a host of different fit criteria that try and approximate bias incurred by overfitting:

   Fit = within sample fit + complexity penalty


   The most well known:
   - Bayesian Information Criterion -- penalizes for # of parameters and N
   - Akaike Information Criterion -- penalizes for # of parameters 

   Not available for most ML algorithms and only useful in a relative sense. They must compute a log likelihood ($ln(\text{likelihood})$).

** DONE Homework Assignment 2
   DEADLINE: <2019-09-19 Thu 09:30>
* Week 4: Regression
** Classification Metrics
   <2019-09-17 Tue 09:30-10:45>
*** Classification Definition

    Classification = modeling a categorical outcome
    - binary
    - multicategorical

    Binary is far more common, and most metris will generalize to multicategorical outcomes.

    Most algorithms output a predict probbability of belonging to class 1. - Clustering and others = direct assignment to class

*** Goals
    Instead of focusing on objective functions for categorical outcomes, our goal is to answer the following question: How well did our model do in predicting a categorical outcome?

    In this, we run various algorithms and create predictions. Then comparing predictions to the actual outcomes.

*** Accuracy

    $$\text{Accuracy} = \frac{1}{n}\sum_{i=1}^{n}I(y_{i}=\hat{y}_{i})$$

    What percentage of observations do we accurately classify?
    
*** Example
    We are going to try and model whether someone has endorsed depressive symptoms.
    - Use a cutscore on the BDI from the epi.bfi dataset in the psych package.

    Just use the five Big Five Inventory (BFI) scores as predictors.
    - Agreeableness, Conscientiousnous, Extraversion, Neuroticism, Openness

*** Predicted Probabilities

    [[./L_Pics/Pred_Class_Model.png]]

*** Calibration Plot

    [[./L_Pics/Calib_Plot_1.png]]

*** Confusion Matrix

    [[./L_Pics/Conf_Mat_1.png]]

    Sensitivity can be thought of as the total number of predicted positive cases divided by the /actual/ number of positive cases.

    Specificity is the counterpart of sensitivity: it is the total number of predicted negatives divided by the /actual/ number of negatives.


    |                   | Actual Outcome |              |               |
    |                   | Positive       | Negative     |               |
    |-------------------+----------------+--------------+---------------+
    | Predicted Outcome | Positive       | TP           | FP ($\alpha$) |
    |                   | Negative       | FN ($\beta$) | TN            |

*** Balanced Accuracy
    <2019-09-19 Thu 09:30-10:45>
    
    This gives you the average across both classes.

    |            | Actual |     |
    | Prediction |    Yes |  No |
    |------------+--------+-----|
    | Yes        |     14 |   8 |
    | No         |     39 | 170 |
    
    - For Yes: $14/(14+39) = 0.26$
    - For No: $170/(170+8) = 0.955$
    - Average = 0.61

      [[./L_Pics/Sens_and_Spec.png]]

*** Kappa
    We can eyeball or use other statistics to compare Accuracy to the no information rate (1-prob(class 1)), but there are other statistics that measure our ability to predict beyond chance.

    $$\text{Kappa} = \frac{O - E}{1 - E}$$

    Where $O$ is the observed Accuracy and $E$ is the expected accuracy based on the marginal totals.

    $$E =  \frac{(Predicted_{No}*Actual_{No})}{N^2} + \frac{(Predicted_{Yes}*Actual_{Yes})}{N^2}$$

    Values > 0 mean an improvement above and beyond chance.

*** Creating Cutoffs

    If you don't specify, most programs will use a cutoff of 0.5. Which, in many cases, makes sense.

    How about in our example? Our baserate is 0.22.

** Probability Metrics

*** Area Under the Curve
    The area under the receiver operating characteristic curve (AUC) can be characterized as the probability that a randomly drawn positive case has a higher probability than a randomly drawn negative case (Fawcett, 2006).

    [[./L_Pics/AUC1.png]]
    
    For this picture, the gradient is your selected cutoff rate for classifying outcomes. Sensitivity is how well it identifies positive cases. FPR is your false positive rate i.e. how frequently you erroneously predict a positive when it is negative.

    An AUC of 1 means you have perfect specificity and sensitivity i.e. you always predict correctly. An AUC of 0.5 would mean you are doing the as well as random chance would e.g. a line from one corner to the other. Less than 0.5 means you are doing /worse/ than random chance.

    Here's an example of the area under a curve for two models: Logistic and Random Forest. RF fits perfectly because there was no cross-validation.

    [[./L_Pics/AUC2.png]]

*** Problems with AUC
    Class Imbalance is a major drawback (Saito \& Rehmsmeier, 2015)
    - High imbalance will inflate the AUC

   In the suicide example before, our AUC was 0.99 in most models.
   - Even though we only found tiny effects

   Also, need a test set to get actual curve.
   - Can calculate as part of 5-fold

*** PRAUC

    Let's invert our approach: how well do we predict the positives?

    [[./L_Pics/PRAUC.png]]
    
    Unlike AUC, there is no hard and fast value to look for e.g. 0.5 is equivalent to chance. PRAUC is all relative to other models.

** Introduction to Regularization

*** Variable Selection
    Regression uses the predictors to estimate $b_j$ to maximize $R^2$. But this is within sample.

    In some scenarios, removing predictors will improve our ability to accurately estimate $_TRS^2$.

    Earlier we used CV and bootstrapping to create a better estimate of $_TRS^2$, now we want to use variable selection with CV/bootstrapping to maximize $_TRS^2$.

*** Stepwise Methods: Best Subsets Selection

    To perform best subsets selection our goal is to find the /optimal/ configuration of variables, testing /all/ possible configurations.

    Number tested equals $2^k$, where $k$ is number of variables.

    Possible configurations:
		- 5 variables = 32
		- 20 = 1,048,576
		- 100 = 1.267651e+30

    Point being? It can take a while. So we need heuristics.

*** Backward Elimination

    Backwards, start with the full model (all predictors).

    In each step, remove the variable that lowers $R^2$ the least.

    The key: What metric to use to choose a final model?

*** Information Criteria
    
    Two really popular information criteria:

    Bayesian Information Criteria (BIC; Schwarz, 1978)

    $$BIC = N*log(RSS/N) + k*log(N)$$

    Akaike Information Criteria (AIC; Akaike, 1973)

    $$AIC = N*log(RSS/N) + 2k$$

    Which is equivalent to Mallow's Cp in regression.

    The lower the better. Both have similar idea to $R^2_{adj}$, but can also be used in non-nested models (but need same people).

    E.g.

    [[./L_Pics/Backwards.png]]

*** Selecting Final Model
    Pick the simplest model with the lowest BIC e.g. Intercept, C2, C3, E3, E4, N2, O2, gender.

*** Holdout
    In using stepwise selection, do not make inferences on the sample used to select variables! P-values are no longer valid. If you have a holdout, you can test the new subsetted model on the holdout and make inferences.

    Thus, I re-run the model on the test set and can examine p-values. 

    No test set = no p-values. Sorry

*** Forward Stepwise Regression
    Start with only an intercept, select the variable that has the largest correlation with Y (positive or negative).

    Next, add variable that increase $R^2$ the most. etc... 

    You can constrain it to have requirement that improvement is significant, but what do p-values mean anymore...

    Applied to our example, we get the same results.

*** A quote on stepwise methods:
    "Stepwise variable selection has been a very popular technique for many years, but if this procedure had just been proposed as a statistical method, it would most likely be rejected because it violates every principle of statistical estimation and hypothesis testing." (Harrel, 2015, p. 67).

*** Regularization
    Stepwise methods are not necessarily deficient, it is that the procedure invites too many opportunities for abuse.

    In contrast, we will discuss a number of methods under the umbrella of a method that is termed "regularization."

    This is a family of methods designed specifically for when the number of predictors grows beyond just a few.

*** Ridge Regression

    Ordinary least squares can be represented as minimizing the residual sum of squares (RSS):

    $$RSS = argmin\Big\{\sum_{i=1}^{N}(y_{i}-b_{0}-\sum_{j=1}^{p}x_{ij}b_{j})^{2}\Big\}$$

    From this, we build a penalty term:

    $$ridge=\underbrace{RSS}_{OLS}+\underbrace{\lambda\sum_{j=1}^{p}b_{j}^{2}}_{ridge}$$

*** Lasso Regression
    Specifically to /select/ variables, the least absolute shinkage and selection operator (lasso; Tibshirani, 1996, 2011) is:

    $$lasso=\underbrace{RSS}_{OLS}+\underbrace{\lambda\sum_{j=1}^{p}|b_{j}|}_{lasso}$$

*** Alternative Equation

    $$lasso=\underbrace{RSS}_{OLS} \quad \text{subject to} \quad \sum |b_j| \le t$$

    $$ridge=\underbrace{RSS}_{OLS} \quad \text{subject to} \quad \sum b_j^2 \le t$$

    where $t$ is a tuning parameter similar to $\lambda$

*** Example 1

    [[./L_Pics/Reg_1.png]]

*** Example 2

    [[./L_Pics/Reg_2.png]]

*** Parameter Trajectory

    [[./L_Pics/Reg_3.png]]

*** Collinearity
    If I simulate two variables, $x_1$ and $x_2$, to both have $b_j$ of 1, but a correlation between them of 0.999, this is what occurs in linear regression.

    [[./L_Pics/Reg_4.png]]

*** Ridge for Collinearity
    
    |   | $\lambda$ |   x1 |   x2 |
    |---+-----------+------+------|
    | 1 |      0.00 | 1.44 | 0.15 |
    | 2 |       0.1 | 1.15 | 0.44 |
    | 3 |       0.2 | 1.04 | 0.55 |
    | 4 |       0.3 | 0.98 | 0.60 |
    | 5 |       1.0 | 0.86 | 0.72 |
    
    By penalizing both coefficients, it /shrinks/ the inflated coefficients and inflates the suppressed varaibles.

    In general, no easy remedy for collinearity.

*** Main Points

    Ridge regression:
    - Handles collinearity.

    Lasso regresson: 
    - Performs variable selection.

    As a way to combine these, Zou and Hastie (2005) proposed the /elastic net/ (enet) regularization. Through the use of a mixing parameter, $\alpha$, the elastic net combines both ridge and lasso regularization

    $$enet= \underbrace{RSS}_{OLS} + \underbrace{(1-\alpha)\lambda\sum_{j=1}^{p}\beta_{j}^{2}}_{ridge} + \underbrace{\alpha\lambda\sum_{j=1}^{p}|\beta_{j}|}_{lasso}$$

*** Elastic Net Note

    One drawback: generally not as sparse as the lasso
    
    Also, more computationally intensive.
    - Could just test value of 0.5
    - Or 0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1

    Typically worth doing manually with ~train()~.
    - glmnet won't test vector
      
*** Example Script from Prof. Jacobucci

    #+begin_src R
#ecls.1 = read.table('C:/Users/rjacobuc/Documents/GitHub/edm_book/ch5_trees/ecls_DM.dat', na='.');
#ecls.1 = read.table('/Users/rjacobuc/Documents/GitHub/edm_book/ch5_trees/scripts/ecls_DM.dat', na='.');
#ecls.1 = read.table('C:/Users/rjacobuc/Documents/GitHub/edm_book/ch5_trees/scripts/ecls_DM.dat', na='.');

#library(synthpop)

#ecls_syn <- syn(ecls.1)$syn
#write.csv(ecls_syn,"G:/My Drive/PSY-ML-Fall19/4.Scripts/ecls_syn.csv")

ecls_syn <- read.csv(file.choose())

names(ecls_syn) = c('gender','kage',
                  'k_read_irt','k_read1','k_read2','k_read3','k_read4',	
                  'k_print','k_read_tht',
                  'k_math_irt','k_math1','k_math2','k_math3','k_math4',
                  'k_math_tht',
                  'k_gk_irt','k_gk_tht',
                  'f_mtr','g_mtr',
                  'P1LEARN','P1CONTRO','P1SOCIAL','P1SADLON','P1IMPULS',	
                  'ars_lit','ars_mth','ars_gk',
                  'T1LEARN','T1CONTRO','T1INTERP','T1EXTERN','T1INTERN',	
                  'height','weight','bmi',
                  'hisp','na_amer','asian','black','pac_isl','white','m_race',
                  'ses_c','ses_cat','poor','income',
                  'g8_read','g8_read_tht','g8_math','g8_math_tht',
                  'g8_sci','g8_sci_tht')

# subset of variables
x.vars = c('k_math_irt','k_gk_irt',"income","bmi","gender")
set.seed(1)
x.noise <- matrix( rnorm(nrow(ecls_syn)*20,mean=0,sd=1), nrow(ecls_syn), 20) 
y.vars = 'g8_sci'
ecls_syn$gender = ecls_syn$gender - 1
XX <- data.matrix(cbind(ecls_syn[,x.vars],x.noise))
XX.std <- data.matrix(cbind(scale(ecls_syn[,x.vars]),x.noise))
YY <- as.numeric(scale(ecls_syn[,y.vars]))



# linear regression
dat.comb <- data.frame(YY,XX.std)
lm.out <- 






library(glmnet)
# ?glmnet
# return in original scale
lasso.out <- glmnet(XX,YY,family="gaussian",alpha=1)
plot(lasso.out)

lasso.cv <- cv.glmnet(XX,YY,family="gaussian",alpha=1)
plot(lasso.cv,size=2)

round(coef(lasso.cv,lasso.cv$lambda.1se),3)
round(coef(lasso.cv,lasso.cv$lambda.min),3)


relax.out <- lm(scale(g8_sci) ~ scale(income) + scale(k_math_irt) + scale(k_gk_irt),ecls_syn)
summary(relax.out)
round(coef(relax.out),3)

# ridge

ridge.out <- glmnet(XX,YY,family="gaussian",alpha=0)
plot(ridge.out)

ridge.cv <- cv.glmnet(XX,YY,family="gaussian",alpha=0)
plot(ridge.cv)
round(coef(ridge.cv,ridge.cv$lambda.1se),3)


#lasso.std <- glmnet(XX.std,YY,family="gaussian",alpha=1,intercept=F,standardize=F)
#plot(lasso.std)

# penalized package 
#library(penalized)
#fit1 <- penalized(YY, penalized=XX, unpenalized=~0, standardize=TRUE) 
#round(coefficients(fit1),3)


# lasso p-values

library(covTest) # have to install from cran arxiv https://cran.r-project.org/src/contrib/Archive/covTest/
a=lars.en(XX,YY,lambda2=0)
cov.out = covTest(a,XX,YY)
cov.out$results[1:4,1] # get predictor numbers

head(XX[,c(2,1,3,5)])




# hierarchical lasso
set.seed(1)
ids <- sample(1:nrow(XX.std),nrow(XX.std)*.5)
XX.train <- XX.std[ids,]
XX.test <- XX.std[-ids,]
YY.train <- YY[ids]
YY.test <- YY[-ids]

library(hierNet)

out = hierNet.path(XX.train,YY.train,strong=TRUE)
out.cv = hierNet.cv(out,XX.train,YY.train)

plot(out.cv)
out.cv

out$th[,,11] # interactions and quadratic effects
out$bp[,11] # positive main effects
out$bn[,11] # negative main effects

# test if weak identifies others.
out.weak = hierNet.path(XX.train,YY.train,strong=FALSE)
out.cv.weak = hierNet.cv(out.weak,XX.train,YY.train)
plot(out.cv.weak)
out.cv.weak

out.weak$th[,,11] # interactions
out.weak$bp[,11]
out.weak$bn[,11]


# re-run model

dat.comb <- data.frame(XX.test,YY.test)
colnames(dat.comb)[1:5] <- c("math","knowledge","income","bmi","gender")
colnames(dat.comb)[26] <- "science"

lm.int <- lm(science ~ math + knowledge + income + bmi + gender + 
               I(math^2) + I(knowledge^2) + I(income^2) + I(bmi^2) +
               math*knowledge + knowledge*income,
             dat.comb)
summary(lm.int)
stargazer::stargazer(lm.int,column.sep.width = "1pt",single.row = TRUE)




# try stability selection

# stability
library(stabs)
stab.out11 <- stabsel(XX,YY,cutoff=0.75,PFER=1)
stab.out11
    #+end_src

** Regularization and Bias-Variance
*** APE Exposure Data
    [[./L_Pics/APE_Data.png]]
*** 20th Order Polynomial
    [[./L_Pics/Reg_5.png]]
*** Random Forests
    [[./L_Pics/Reg_6.png]]
*** Bias vs. Variance
    [[./L_Pics/Bias_v_Var.png]]
*** Fit Decomposition
    $$MSE = Bias^{2} + Variance$$

    $$\mathop{\mathbb{E}} \left[ (y - \hat{f}(x))^2  \right] = \left(Bias[\hat{f}(x)] \right)^2 + Var[\hat{f}(x)] + \sigma^2$$

    $$Bias[\hat{f}(x)] = \mathop{\mathbb{E}}[\hat{f}(x)] - \mathop{\mathbb{E}}[f(x)]$$

    $$Variance[\hat{f}(x)] = \mathop{\mathbb{E}}[\hat{f}(x)^2] - \mathop{\mathbb{E}}[f(x)]^2$$

    Its about decomposing the expected error on an unseen sample (population).

    - Overly simple model = High Bias
    - Overly complex model = High variance
*** Decomposing

    [[./L_Pics/Decomp.png]]
*** Model Fit
    [[./L_Pics/Model_Fit.png]]
*** OLS vs. Regularization

    Simulate data acording to a linear regression -- run model on sample size of 40.
    [[./L_Pics/OLS_v_Reg.png]]
*** Sample Size

    The Bias-Variance Tradeoff is typically discussed with a fixed N.

    As sample size decreases, variance of fixed model will increase.
    - Less information to model
    - Conclude simpler model with fit better
*** Regularization and Sample Size

    [[./L_Pics/Reg_and_Samp.png]]

*** Sample Size Continued

    Example evaluated on test set.
    - Why more data can beat better algorithms

    [[./L_Pics/Samp_Size.png]]
    
*** Conceptual Quiz

    What are the bias and variance properties of the following algorithms. Detail what data generating scenarios for which this holds:

    - 20th degree polynomial vs linear regression
    - Linear Regression vs. ridge regression

*** Big Picture Takeaways
    Why are we bothering with this?
    - We can't see bias or variance, so we need to consider this before throwing algorithms at data. Otherwise, you have a recipe for poor prediction/capturing of trends.

    When to prefer bias?
    - When you have few observations e.g. less than 1000
    - When you want to eliminate variables

    When to prefer variance?
    - If you have a lot of observations

* Week 5: Regression (cont.)
** Regularization: Advanced Methods
   <2019-10-01 Tue 09:30-10:45>

*** Lasso P-values

    Simply put: researchers like p-values. With the lasso, it is not recommended to do:
    - bootstrap within the algorithm to get p-values
    - assess p-values in a follow-up step

    Lockhart, Taylor, Tibshirani, and Tibshirani (2014) proposed a new distribution for the test statistic.
    - implemented in ~selectiveInference~ package (this was the ~covTest~ package previously)
      
    In this, use p-value for selection, not whether non-zero.

*** Stability - Variable Selection and Stability
    Variable slection is inherently unstable. Two factors that play a large role:
    1. Sample size
    2. Signal in predictors
       
    If you have small effects among your predictors, you are going to have problems with stability. More specifically, sometimes the lasso will select a variable as being non-zero, and sometimes it will not. This is not unique to the lasso; the same problem exists for other methods.

*** Stability Selection

    As opposed to just bootstrapping the variable selection process, Stability Selection (Meinshausen & Buhlmann, 2010) developed a more formal process. It involves:
    - Taking at least 100 bootstrap samples (or subsamples) for each penalty
    - Selecting a subset of the penalty range to asses selection probabilities for each predictor
    - Range of penalites is based on eseeting a false positive error rate
      
    N.B. you will set your own threshold. 0.8 or 0.9 are generally pretty good standards.

    [[./L_Pics/Ad_Reg_2.png]]

    [[./L_Pics/Ad_Reg_1.png]]

*** Moderation Analysis
    Consider a model
    $$Y_i = b_0 + b_XX_{i} + b_ZZ_{i} + \epsilon_i$$ where $Z$ is a potential moderator. To determine whether  $Z$ moderates the effect of $X$ on $Y$ we can fit $$Y_i = b_0 + b_XX_{i} + b_ZZ_{i} + b_{XZ}X_iZ_{i} + \epsilon_i$$ and test the hypothesis that $$b_{XZ}=0$$

    If $$b_{XZ} \neq 0$$, we can claim that $Z$ moderates the relationship between $X$ and $Y$.

    Equivalently, we can say that there is an /interaction/ between $X$ and $Y$ or that the relationship between $X$ and $Y$ is conditional on $Z$.

*** Parameter Interpretation

    $$Y_i = b_0 + b_X(X_{i}-\bar{X}) + b_Z(Z_{i}-\bar{Z}) + b_{XZ}(X_{i}-\bar{X})(Z_{i}-\bar{Z}) + \epsilon_i$$

    [[./L_Pics/Ad_Reg_3.png]]

    [[./L_Pics/Ad_Reg_4.png]]

*** Searching for Interactions

    A number of methods have been developed for high-dimensional selection of interactions: (Bien, Taylor, & Tibshirani, 2013; Choi,Li, & Zhu, 2010; Haris, Witten, & Simon, 2016).

    $$Y=\beta_{0}+\sum_{j}\beta_{j}X_{j}+.5\sum_{j\neq k}\Theta_{jk}X_{j}X_{k}+\epsilon$$

    $\Theta_{jk}$ is a matrix assessing every possible interaction between variables. 

    In R, there are the ~hierNet~ and ~FAMILY~ packages. Both are slow. 

    Have to choose an assumption to follow:
    Strong hierarchy -- Only test interactions if both variables have simple effects.
    Weak hierarchy -- Have to have one non-zero simple effect.

    N.B. Quadratic effects can be masked by interactions and vice versa.

*** Example Results

    [[./L_Pics/Ad_Reg_5.png]]

*** Hierarchical Testing

    When we have /sets/ of predictors, we are often interested in knowing whether the whole set is related to the outcome.

    Note that this should be technically called /Hierarchical testing/, /analysis/, or /entry/ to differentiate from Hierarchical/Multilvel/Mixed models.

    This form of testing represents a /confirmatory/ analysis. The steps are determine /a priori/. 

    What variables and in what steps/order is determined theoretically. This method can be tailored to fit what is of theoretical interest to you.

*** Example Hypothesis

    I hypothesize that conscientiousness will predict agreeableness even after controlling for demographic variables. 

    Model 1: $ agree = b_0 + A*demo$
    Model 2: $agree = b_0 + A*demo + B*conscientiousness$

*** Additional Example

    I wish to test the following sequence

    Model 1: $agree = b_0 + A*demo$
    Model 2: $agree = b_0 + A*demo + B*consc$
    Model 3: $agree = b_0 + A*demo + B*consc + C*open$
    Model 4: $agree = b_0 + A*demo + B*con + C*open + D*neur$

    Which produces the following relationship:

    $R^2(Model 4) = \Delta R^2(Model 4) + \Delta R^2(Model 3) + \Delta R^2(Model 2) + R^2(Model 1)$
    
*** Selecting Among Groups

    For a scenario where the numbers of variables preclude strong theoretical specification, Yuan and Lin (2007) proposed the group lasso

    $$grplasso=argmin\Big\{\sum_{i=1}^{N}(y_{i}-\beta_{0}-\sum_{j=1}^{p}x_{ij}\beta_{j})^{2}+\lambda\sum_{g=1}^{G}\sqrt{p_{g}}\|\beta_{g}\|_{2}\Big\}$$

    where $p_{g}$ accounts for the varying sizes of groups and $\beta_{g}$ is the vector of all $\beta$ coefficients in group $g$. In the group lasso, the lasso penalty acts at the group level, where instead of individual parameters are set to zero, entire groups of parameters can be pushed to zero.
** DONE Homework Assignment 3
   DEADLINE: <2019-10-03 Thu 09:30>
* Week 6: Trees
** Decision Trees
   <2019-10-03 Thu 09:30-10:45>
*** Intro
    Decision Trees (DT) were first introduced in the 1960's by Morgan and Sonquist (1963). However, the use of the method did not capture the attention of the statistics community until 20 years later with the work of Breiman, Friedman, Olshen, and Stone (1984).

    Also referred to as:
    - Recursive partitioning
    - CART
    - Classification and regression trees
*** Algorithm Overview
    First, divide the predictor space into all possible unique values. Across these unique predictor values, the algorithm iteratively partitions the dataset to determine an improvement in model fit. This involves placing each observation into one of two groups, of which, each group member receives the same predicted response.

    Given /m/ unique sorted values, this involves testing /m-1/ unique splits. For nominal variables, as there is no ordering to the values, all possible groupings are tested. If there are /m/ categories, this involves testing 2^{m-1}-1 dichotomous splits.

    For continuous outcomes, this is generally mean squared error. For categorical, it can be accuracy or two alternatives such as entropy or the gini index.

    It then checks each new group and sees if it can improve the fit by splitting the data further.
*** Example Tree
    [[./L_Pics/D_Tree_1.png]]
    
    Each split is created by testing each predictor variable and checking every internal partition within that variable to improve the fit. In other words, splitting the "worthless" variable at 0.5 is the /best/ way of reducing the error for the first data split.
*** Terminology
    [[./L_Pics/D_Tree_2.png]]
*** Prediction Line (or, relationship between 1 predictor and 1 outcome)
    [[./L_Pics/D_Tree_3.png]]
*** Prediction Surfaces (or, 2 predictors for 1 outcome)
    [[./L_Pics/D_Tree_4.png]]
*** Interactions
    [[./L_Pics/D_Tree_5.png]]
** Decision Trees II
   <2019-10-08 Tue 09:30-10:45>
*** Algorithms
    The term DT often refers to the use of classification and regression trees. It denotes the overarching methodology. Classification and Regression Trees (CART; Breiman et al., 1984) refers to a specific algorithm that falls under the umbrella of DT.

    Two different packages implement variants
    - rpart
    - tree

    Often time this tree structure fits the data too well (overfits), meaning that parts of the tree structure are unlikely to generalize well to alternative samples. One strategy to overcome this prpoensity is to _prune_ the initial tree back to a smaller size.
*** Pruning
    In choosing a final model, it is common to build a large tree and then prune back the tree to select a sub-tree i.e. a smaller version the tree that minimizes the cross-validation error. This is done in order to prevent "missing" an important additional split (see Breiman, Friedman, Olshen, and Stone, 1984).

    In practice, pruning involves testing multiple tuning parameters that control the size of the tree, using CV to select among these. Conversely, we can create a tree without attempting to control the size of it, the proceed to _prune_ back the leaves to create a series of submodels/subtrees. Both typically result in the same tree.

*** Cost Complexity

    The cost complexity measure is:

    $$R_{\alpha}(T_{p})=R(T_{p})+\alpha s_{p}$$

    where $R(T_{p})$ is the error, and $s_{p}$is the number of leaves (terminal nodes) for tree $T_{p}$(Breiman et al., 1984). When $\alpha$ is zero, then we have the original tree structure.

    What does this look similar to? Regularization. This is the same thing but for decision trees.

*** Example Results
    | CP | Splits | Sample Error | Avg. CV Error | CV Error Std. |
    |----+--------+--------------+---------------+---------------|
    |  1 |      0 |            1 |             1 |          0.03 |
    |  2 |      1 |         0.83 |          0.83 |          0.03 |
    |  3 |      3 |         0.77 |          0.84 |          0.03 |
    |  4 |      5 |         0.74 |          0.82 |          0.03 |
    |  5 |      6 |         0.73 |          0.83 |          0.03 |

    Table 1: Table of complexity parameters and fit from Example 1.
    
    Note: there are big benefits to having small trees e.g. stability. But... readers may not trust your results. They may assume that they are not receiving all of the information. It's up to you to decide what to report.

*** Example cont.

    To demonstrate this process we can use the ECLS-K data. This time, we predict reading achievement in grade 8. The sample was split into both a train and test set. For each tree, we recorded the fit on the train set, the average fit across the 10 folds, and on the test set.

    [[./L_Pics/D_Tree_6.png]]

*** Conditional Inference Trees

    Conditional Inference Trees (Ctree; Hothorn , hornik, & Zeileis, 2006) are based on a general theory of permutation tests, performing a hypothesis test at each node resulting in a p-value criterion to test whether the tree should stop or keep growing. Using a perutation test to calculate a p-value entails comparing a split on the original sample to using that same split on randomly shuffled response values (e.g. swapping observation 1's and 2's response).

    This overcomes a well known problem with CART (Kin & Loh, 2001). It does not control for the number of response options.

*** Partykit package

    The algorithm is iimplemented as the ~ctree~ function in the ~partykit~ package (Hothorn & Zeileis, 2015). The default for the p-value criterion is 0.05 (expressed as 0.95, or 1-p).

    Can use in ~train()~ with ~method="ctree"~ . Using the default tuning parameters (3) is pretty reasonable, while when using ~method = "rpart"~, its best to test 20-30 or so.

*** Stability
    
    Example:

    To make the concept more concrete, we will use the ~Holzinger-Swineford~ dataset from the ~lavaan~ package (Rosseel, 2012). With this dataset, we created three trees using the ~rpart~ package, one on the first 100 respondents, one of the second 300 respondents, and finally on the entire sample (N=301). The resulting tree structures for each are randomly different but they seem to reduce our error by commesurate levels.

    [[./L_Pics/D_Tree_7.png]]

    Adding backward selection to choose which variables should be in the model (usiing the Akaike Information Criterion to choose a final model), we get additional uncertainty. In the first sample, variables x4 and x5 are chose in the second sample x3, x4, and x6, while in the full sample x2 and x4.

    Unsurprisingly, small trees are more stable. However, this comes at the cost of prediction quality and inference. Similar concept to bias and variance, except here there is high variability to the interpretation when bias is low.

*** Measuring Stability

    If inference is of high priority, it is worth quantifying how unstable a tree is.

    The easiest solution is using 100 bootstrap samples to see how often the same tree is created.

    The is made relatively easy with the ~dtree~ package.

    |       | nodes       | nvar | nsplits | RMSE CV | $R^2 CV$ |
    |-------+-------------+------+---------+---------+----------|
    | lm    | -           |    - |       - |   11.75 |     0.37 |
    | rpart | 15.42 (4.4) | 2.00 |   14.42 |   11.51 |     0.39 |
    | ctree | 22.67 (9.4) | 2.00 |   21.67 |   11.48 |     0.40 |

*** Additional Information: Surrogate Splits

    Two options for handling missingness in trees:
    - Multiple imputation
    - Surrogate splits

    Surrogate splits work in the following way: after a primary split is found for a given node, surrogate splits can be found by re-applying the partitioning algorithm to predict the binary primary split. For example, if the primary split is on education between less than or equal to 12 years and greater than 12, then this new binary variable becomes the outcome, with the remaining variables used as predictors.

*** Variable Importance

    Similar to that of linear regression, having high correlations among covariates presents problems for DT. In the tree building algorithm, at a given split two gollinear variables may produce almost idential improvements in fit, but only one can be chosen for a given split. This is analogous to the idea of masking, but in the case of DT, it results in one of the variables either not being split on at all, or lower in the tree. To quanitfy how influential a variable is in predicting the outcome variable, one can also calculate the varaible importance metric for a given tree.

    Add collinear variable messes up the tree, but variable importance were as follows: 39 for math, 33 for knowledge, and 28 for the simulated math score.

*** Summary

**** Advantages:

     - Are robust to outliers - single misclassifications don't largely influence the splitting.
     - Perform variable selection
     - Genrally result in easy to interpret tree structures
     - Automatically include interaction 

**** Disadvantages:
     - Instability
     - Collinearity presents problems
     - Relatively lower predictive performance

*** DONE Homework Assignment 4

** Tree Recap
*** Surrogate Splits
    What are they?
    - First, we produce a split for a node. What happens if our predictor is missing values? We can check all variables to see what variable is the best predictor of the original predictor. For example, say our original variable is ~MATH~. However, we are missing values. But of our predictors, ~IQ~ serves as the best predictor of ~MATH~. So for our missing ~MATH~ values, we check how we split our observations if we had split based on ~IQ~ and then put individuals in each category into the original groups for ~MATH~ i.e. if our outccomes are 0 and 1, anyone predicted to be a 0 for ~IQ~ is put into the predicted 0 node for ~MATH~.
* Week 7: Ensembles
** Bagging
   <2019-10-10 Thu 09:30-10:45>
*** Downfalls to Decision Trees

    We've already discussed some of the drawbacks to decision trees, namely:
    - Poor predictive performance
    - Instability in structure
    - Bias in variable selection (CART)

    Here, we are going to discuss one of the main algorithms proposed to overcome these problems: random forests. Prior to discussing RF, we will look at bagging, a precursor.

*** Bagging Overview

    One of the first methdos proposed to overcome the limitations of decision trees is bootstrap aggregating, or bagging (Breiman, 1996).

    The algorithm proceeds in two steps. First, a bootstrap sample is taken. Then for each bootstrap sample, fit a tree model using all predictors. Given that each bootstrap sample is different, each tree will tend to have a unique structure as well, capitalizing in the instability of decision trees, manifested in either the variables used, splitting values, or tree size.

*** Bagging Predictions

   $$\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^{B}\hat{f}^{*b}(x)$$ 

   ... where our predictions $\hat{f}_{bag}(x)$ are take as the average predictions across each tree $\hat{f}^{*b}(x)$ for B number of trees created, with B being approximately 500. In the case of a categorical outcome, we can use majority vote across the trees.

   By averaging the prediction across 100s of trees, we reduce the variance.

*** Example

    Using the PHE Exposure data, we start with a single tree.

    1 Tree:
    [[./L_Pics/RF_1.png]]
   
    5 Bagged Trees:
    [[./L_Pics/RF_2.png]]
    
    2000 Bagged Trees:
    [[./L_Pics/RF_3.png]]

*** Testing Different Numbers of Trees

    Using the same example, we can see how our predictive performance changes across an increasing number of trees.

    [[./L_Pics/RF_4.png]]
    
*** Drawbacks to Bagging

    When using the same set of predictors to create each of the trees, many of the trees will be overly similar (or the same), resulting in a limited search of the prediction space. Bagging reduces the variance of individual trees, but the bias is approximately the same since the same trees are used. Put more concisely, the trees from bagging typically have high correlations.

    Random forests was proposed ot keep the low variance of bagging, but further reduce the bias by de-correlating the trees. This is achieved by randomly selecting a subset of the variables for the creation of each tree.

** Random Forests
*** Algorithm
    
    *for* b = 1 to B *do*
    1. Draw a boostrap sample of size N from training data
    2. Grow a decision tree $T_b$ on the bootstrap sample using $m$ variables
    *end*
    *Result:* Output the ensemples of trees $\{T_B\}^B_1$
    
    To create predictions:

    $$\hat{f}_{rf}^{B}(x)=\frac{1}{B}\sum_{b=1}^{B}T_{b}(x)$$
    
    Note that some versions of random forests use subsamples to create trees, not boostrap samples.

*** ~mtry~

    ~mtry~, or the number of randomly selected variables for creating each tree, is the main tuning parameter in random forests. You can vary the number of trees, but it typically doesn't matter for prediction (different for variable importance).

    Default values in most programs:
    - Classification is $sqrt{p}$
    - Regression is $p/3$

*** Out of Bag Samples

    When using bootstrap samples, can use of the out of bag (OOB) samples to create samples. Then, average predictions for each observation just in those trees in which they were used (e.g., (1-.632)*B number of trees).

    Using OOB prediction performance approximates the use of k-fold pretty well. However, it is more common to just use k-fold.

*** Compared to Bagging

    [[./L_Pics/RF_5.png]]
    
    Random forests are, by nature, much more likely to overfit your data.

* Week 8: Ensembles (cont.) and Project Proposals
** Random Forests (cont.)
   <2019-10-17 Thu 09:30-10:45>
*** Variable Importance
    Why not just univariate assessment?
    
    Assessing the importance of all predictors in the same model, as opposed to using univariate models, can capture the multivariable \textbf{interactions}.
    
    Particularly when predictors are correlated, the multivariable assessment can result in different inference than univariate.

    Multivariable assessments can also remove spurious effects in a univariate model -- a variable associated with the outcome only because of its relation to a different predictor.

*** Variable Importance in a Single Tree
    
    From Breiman et al. (1984)

    $$I_{l}^{2}(T) = \sum_{t=1}^{J-1}\hat{i}_{t}^{2}I(v(t)=l)$$

    ... for each predictor $X_l$, $J-1$ internal nodes, each node $t$, and $\hat{i}_{t}^{2}$ is a measure of error improvement.

    Taking the square root gives an importance value for each predictor. 

    What are the issues with this?
    - This only gives non-zero values for variables that were used in the tree. If it wasn't used in the tree, you don't have an accurate measurement of importance.
    - ~rpart~ gets around this by using surrogate splits to approximate this.

*** Variable Importance Across Trees
    In both boosting and random forests, can alter this to:

    $$I_{l}^{2} = \frac{1}{M}\sum_{m=1}^{M}I^{2}_{l}(T_m)$$

    ...where M is the number of trees.

    Key point: this is relative, so common to assign largest value to 100 then scales each accordingly.

*** Permutation Tests

    From Strobl et al. (2008): "that a random permutation of the values of the predictor variable is supposed to mimic the absence of the variable from the model."

    This form of VI was proposed in Breiman (2001): Using the out of bag samples (OOB), for the Mth tree grown, pass the OOB samples down the tree, and record the prediction performance. Then, for the $lth$ variable, permute the values and pass down the new OOB sample data, and record the prediction performance. In contrast to CTree permutation, this is change the actual values of X.

*** Problems with RF VI

    The way of permuting X values in RF can be referred to as a marginal approach. This has been found to bias VI values in favor of predictors that are correlated. As a result, Strobl et al. (2008) developed a conditional approach. 

    For the permutation, add this: "For all variables Z to be conditioned on: Extract the cutpoints that split this variable in the current tree and create a grid by means of bisecting the sample space in each cutpoint."

    Then calculate OOB VI of X *within* each point in the grid.

*** Random Forest Example

    The randomForest package, method="rf", by default uses the permutation test on the OOB approach. Using the ECLS-K data and 100 trees:

		|           |     RF | Rpart |
		|-----------+--------+-------|
		| math      |  17.29 |    13 |
		| knowledge |  28.14 |    13 |
		| weight    |   1.37 |     0 |
		| height    |   0.00 |     0 |
		| income    |  10.95 |     6 |
		| ses       |  10.94 |     8 |
		| read      | 100.00 |    61 |

*** cforest example

    From the partykit package:

    		|           |    CF |
    		|-----------+-------|
    		| math      |  8.81 |
    		| knowledge |  9.98 |
    		| weight    |  0.17 |
    		| height    |  0.27 |
    		| income    |  2.09 |
    		| ses       |  2.46 |
    		| read      | 97.08 |

*** Number of Trees

    For prediction, as long as the number of predictors is small, the number of trees typically doesn't matter. 

    However, for calculating VI, a higher number may be better so there isn't instability to VI.

*** Visualizing the Effects of X

    The idea behind partial dependence plots is to visualize the effect of a variable in a "black box" type algorithm. 

    Broadly speaking, this is examining the effect of $X_1$ on $Y$, after accounting for the average effects of $X_C$. This approach, developed by Friedman (2001) allows for the visualization of interactions and nonlinear effects.

    These are descriptive approximations, not "true" derived relationships.

*** Reading

    Using Random Forests and reading as a predictor:

    [[./L_Pics/RF_6.png]]

*** Math

    Using Random Forests and math as a predictor:

    [[./L_Pics/RF_7.png]]

*** Using Two Predictors

    [[./L_Pics/RF_8.png]]

*** Another Way

    [[./L_Pics/RF_9.png]]

* Week 9: No Class (Fall Break)
* Week 10: Ensembles (cont.)
** Boosting
   <2019-10-29 Tue 09:30-10:45>
*** Introduction
   Similar to random forests, boosting is a method that combines the
   use of hundreds (or thousands) individual decision trees in an attempt
   to overcome some of the problems with single trees. However, in contrast
   to random forests, where each individual tree is created independently
   of every other, boosting creates a sequence of trees based on re-weighted
   forms of the dataset. 

   Developed originally for classification in the
   form of the AdaBoost algorithm (Freund & Schapire, 1995; 1996; 1997),
   the idea behind the algorithm has been extended to many other statistical
   problems (see Buhlmann & Hothorn, 2007 for more detail). Most notably,
   Friedman, Hastie, and Tibshirani (2000), as well as Friedman (2001),
   developed a highly adaptable algorithms that can be applied to a host
   of different loss functions, as well as extended to multivariate outcomes.
*** Algorithm

    The basic idea of this algorithmic framework, originally termed “gradient boosting machines” by Friedman (2001), now more frequently just called “boosting,” is to fit an additive combination of what is termed a “weak learner,” most often a decision tree (weak in the sense that its predictive performance is usually substandard), to the gradient (residuals) of the prior combination of weak learners. More formally, the algorithm is described by:

    [[./L_Pics/Boosting_1.png]]

*** Learning Slowly
    
    This algorithm creates a large sequence of try by continually fitting trees to each current vector of residuals. By shrinking the influence of each tree, and thus how much the residuals are updated, overfitting can be prevented, better known as the process of learning slowly. This is facilitated by the use of the shrinkage parameter (learning rate; $\lambda$), which is usually treating as a tuning parameter, with typically values ranging from 0.001 to 0.1. The smaller the value of $\lambda$, the more trees that could be needed to adequatly fit the data

*** Training Error

    [[./L_Pics/Boosting_2.png]]

*** CV Error

    [[./L_Pics/Boosting_3.png]]
    
*** Number of Trees

    Partially depends on sample size -- smaller = less trees.

    Interaction between learning rate and number of trees -- why testing grid is important. 

    Also, more predictors, need more trees. Somewhere around 500-1000 is typical.

*** Tree Complexity

    The tree depth (complexity) should reflect the order of possible interaction in the data. 

    In social and behavioral research, doubtful to have greater than order 3, however, may be worth testing 5. 

    This means worth testing a depth of: 1 (stump; just main effects; nonlinear effects), 2 can capture interactions, and 3 (higher-order interactions). 

*** Deductive Data Mining

    We developed a method termed deductive data mining: https://osf.io/preprints/bmwah/

    Idea is similar to the Elith et al. paper, use the information from interaction depth to follow up with manual interactions and quadratic effects. 

*** Algorithms
    
    Traditional gradient boosting, implemented in the gbm package, is by far the most popular algorithm. In caret, method = "gbm".  

    A newer algorithm, termed extreme gradient boosting (Xgboost; Chen & Guestrin, 2016) typically has better predictive performance, but the variable importance calculation is a little weird. Implemented in caret as method="xgbTree". 

*** ~gbm~ Package

    gbm has four tuning parameters:
    - number of trees (n.trees)
    - max tree depth (interaction.depth)
    - shrinkage (shrinkage)
    - min. terminal node size (n.minobsinnode)

    In comparison to random forests, boosting takes more tuning. It is worth doing a first run, and then based on results, alter tuning values. 

    Note -- this will also take longer than RF. 

*** Tuning Example

    [[./L_Pics/Boosting_4.png]]

*** Partial Dependence Plots

    Using the ~dismo~ package has additional functionality. One is auto partial dependence plots:

    [[./L_Pics/Boosting_5.png]]

    [[./L_Pics/Boosting_6.png]]

*** Summary

    Gradient boosting with trees is one of the most popular machine learning algorithms. A number of details worth noting when deciding between RF and boosting:
    - boosting often takes longer
    - boosting requires testing more tuning parameters
    - boosting allows for evaluating interactions
    - both can utilize partial dependence plots
    - random forests has been evaluated further with variable importance

* Week 11: Nonlinear methods
* Week 12: Measurment and ML
* Week 13: Searching for Groups
* Week 14: Longitudinal Data
* Week 15: Text Mining
* Week 16: Social Networks
